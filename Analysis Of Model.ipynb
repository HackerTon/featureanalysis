{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 00:54:48.195537: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 00:54:51.330025: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-12-12 00:54:51.515170: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-12-12 00:54:51.515214: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import segmentation_models as sm\n",
    "import functools as ft\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# disable GPU computation\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "sm.set_framework('tf.keras')\n",
    "sm.framework()\n",
    "\n",
    "tf.random.set_seed(1024)\n",
    "SEED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_decode(image, label):\n",
    "    image = tf.io.read_file(image, \"image\")\n",
    "    label = tf.io.read_file(label, \"label\")\n",
    "\n",
    "    image = tf.image.decode_image(image)\n",
    "    label = tf.image.decode_image(label)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# [w, h, c], 448, 448, 3\n",
    "def decode_crop(image, label):\n",
    "    image = image[368 // 2 : -(368 // 2), 256 // 2 : -(256 // 2)]\n",
    "    label = label[368 // 2 : -(368 // 2), 256 // 2 : -(256 // 2)]\n",
    "\n",
    "    img_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for index in range(4 * 8):\n",
    "        x, y = index // 8, index % 8\n",
    "        img_array.append(image[448 * x : 448 * (1 + x), 448 * y : 448 * (1 + y)])\n",
    "        label_array.append(label[448 * x : 448 * (1 + x), 448 * y : 448 * (1 + y)])\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((img_array, label_array))\n",
    "\n",
    "\n",
    "def get_mask(image, label):\n",
    "    labels = []\n",
    "    labels.append((label[:, :, 0] == 0) & (label[:, :, 1] == 0) & (label[:, :, 2] == 0))\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 0) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 64) & (label[:, :, 2] == 128)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 0) & (label[:, :, 1] == 128) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 128) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 64) & (label[:, :, 1] == 0) & (label[:, :, 2] == 128)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 192) & (label[:, :, 1] == 0) & (label[:, :, 2] == 192)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 64) & (label[:, :, 1] == 64) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    # must perform this\n",
    "    return image, tf.transpose(labels, [1, 2, 0])\n",
    "\n",
    "\n",
    "def create_ds(batch_size, istrain=True, maximage=False):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    if istrain:\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_train/**/Images/*.png\"\n",
    "        images = glob.glob(directory, recursive=True)\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_train/**/Labels/*.png\"\n",
    "        labels = glob.glob(directory, recursive=True)\n",
    "    else:\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_val/**/Images/*.png\"\n",
    "        images = glob.glob(directory, recursive=True)\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_val/**/Labels/*.png\"\n",
    "        labels = glob.glob(directory, recursive=True)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    ds = ds.cache()\n",
    "    if istrain:\n",
    "        ds = ds.shuffle(6400, SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(get_image_decode, AUTOTUNE)\n",
    "\n",
    "    if not maximage:\n",
    "        ds = ds.flat_map(decode_crop)\n",
    "\n",
    "    ds = ds.map(get_mask, AUTOTUNE)\n",
    "\n",
    "    # batch and prefetch\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    if istrain:\n",
    "        ds = ds.prefetch(AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_backbone():\n",
    "    _backbone = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False, input_shape=[None, None, 3]\n",
    "    )\n",
    "\n",
    "    outputs = [\n",
    "        layer.output\n",
    "        for layer in _backbone.layers\n",
    "        if layer.name\n",
    "        in [\n",
    "            \"block2a_activation\",\n",
    "            \"block3a_activation\",\n",
    "            \"block5a_activation\",\n",
    "            \"block7a_activation\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[_backbone.input], outputs=outputs, name=\"efficientb0_backbone\"\n",
    "    )\n",
    "\n",
    "\n",
    "class FPN(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(name=\"Feature_Pyramid_Network\", **kwargs)\n",
    "\n",
    "        self.backbone = create_backbone()\n",
    "        self.conv5_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv4_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv3_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv2_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv5_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv5_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv5_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.conv4_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv4_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv4_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.upscale = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        # 112x112, 56x56, 28x28, 14x14\n",
    "        conv2, conv3, conv4, conv5 = self.backbone(images, training=training)\n",
    "        conv5_m = self.conv5_1x1(conv5)\n",
    "        conv5_p = self.conv5_3x3_1(conv5_m)\n",
    "        conv5_p = self.conv5_3x3_2(conv5_p)\n",
    "        conv5_p = self.conv5_bn(conv5_p, training=training)\n",
    "\n",
    "        conv4_m_1 = self.upscale(conv5_m)\n",
    "        conv4_m_2 = self.conv4_1x1(conv4)\n",
    "        conv4_m = conv4_m_1 + conv4_m_2\n",
    "        conv4_p = self.conv4_3x3_1(conv4_m)\n",
    "        conv4_p = self.conv4_3x3_2(conv4_p)\n",
    "        conv4_p = self.conv4_bn(conv4_p, training=training)\n",
    "\n",
    "        conv3_m_1 = self.upscale(conv4_m)\n",
    "        conv3_m_2 = self.conv3_1x1(conv3)\n",
    "        conv3_m = conv3_m_1 + conv3_m_2\n",
    "        conv3_p = self.conv3_3x3_1(conv3_m)\n",
    "        conv3_p = self.conv3_3x3_2(conv3_p)\n",
    "        conv3_p = self.conv3_bn(conv3_p, training=training)\n",
    "\n",
    "        conv2_m_1 = self.upscale(conv3_m)\n",
    "        conv2_m_2 = self.conv2_1x1(conv2)\n",
    "        conv2_m = conv2_m_1 + conv2_m_2\n",
    "        conv2_p = self.conv2_3x3_1(conv2_m)\n",
    "        conv2_p = self.conv2_3x3_2(conv2_p)\n",
    "        conv2_p = self.conv2_bn(conv2_p, training=training)\n",
    "        return conv5_p, conv4_p, conv3_p, conv2_p\n",
    "\n",
    "\n",
    "class FCN(tf.keras.Model):\n",
    "    def __init__(self, n_classes=8, **kwargs):\n",
    "        super().__init__(name=\"FCN\", **kwargs)\n",
    "        self.fpn = FPN()\n",
    "        self.upscale_2x = tf.keras.layers.UpSampling2D()\n",
    "        self.upscale_4x = tf.keras.layers.UpSampling2D((4, 4))\n",
    "        self.upscale_8x = tf.keras.layers.UpSampling2D((8, 8))\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.conv6 = tf.keras.layers.Conv2D(\n",
    "            filters=(512), kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.bnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.conv7 = tf.keras.layers.Conv2D(\n",
    "            filters=n_classes, kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale_final = tf.keras.layers.UpSampling2D(\n",
    "            size=(4, 4), interpolation=\"bilinear\"\n",
    "        )\n",
    "        self.final_activation = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def set_trainable(self, state=False):\n",
    "        for layer in self.fpn.backbone.layers:\n",
    "            layer.trainable = state\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        conv5_p, conv4_p, conv3_p, conv2_p = self.fpn(images, training=training)\n",
    "        m_5 = self.upscale_8x(conv5_p)\n",
    "        m_4 = self.upscale_4x(conv4_p)\n",
    "        m_3 = self.upscale_2x(conv3_p)\n",
    "        m_2 = conv2_p\n",
    "\n",
    "        m_all = self.concat([m_2, m_3, m_4, m_5])\n",
    "        m_all = self.conv6(m_all)\n",
    "        m_all = self.bnorm(m_all, training=training)\n",
    "        m_all = self.conv7(m_all)\n",
    "        m_all = self.upscale_final(m_all)\n",
    "        m_all = self.final_activation(m_all)\n",
    "\n",
    "        return m_all\n",
    "\n",
    "\n",
    "class FCN_ORIG(tf.keras.Model):\n",
    "    def __init__(self, n_classes=8, **kwargs):\n",
    "        super().__init__(name=\"FCN_ORIG\", **kwargs)\n",
    "\n",
    "        self.backbone = create_backbone()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale2x_1 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale2x_2 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale8x = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(16, 16),\n",
    "            strides=(8, 8),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.final_activation = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        _, conv1_o, conv2_o, conv3_o = self.backbone(images, training=training)\n",
    "        conv1_o = self.conv1(conv1_o)\n",
    "        conv2_o = self.conv2(conv2_o)\n",
    "        conv3_o = self.conv3(conv3_o)\n",
    "\n",
    "        fcn_16x = self.upscale2x_1(conv3_o) + conv2_o\n",
    "        fcn_8x = self.upscale2x_2(fcn_16x) + conv1_o\n",
    "        final_output = self.upscale8x(fcn_8x)\n",
    "        final_output = self.final_activation(final_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 00:54:51.694692: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trained_model/new_fpn/ckpt-20'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this iteration is calculated fom 160 iteration from\n",
    "# paper\n",
    "n_classes = 8\n",
    "model = FCN(8)\n",
    "# model = sm.Unet(backbone_name='resnet50', encoder_weights='imagenet', encoder_freeze=False, activation='softmax', classes=8)\n",
    "# model = FCN_ORIG(8)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.012)\n",
    "\n",
    "# load model from the specific model\n",
    "ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "ckptmg = tf.train.CheckpointManager(ckpt, 'trained_model/new_fpn', 5)\n",
    "ckptmg.restore_or_initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JINDEX(out_img, y):\n",
    "    intersection = tf.reduce_sum(out_img * y, [0, 1, 2])\n",
    "    union = tf.reduce_sum(out_img + y, [0, 1, 2]) - intersection\n",
    "    union = tf.maximum(union, 1e-6)\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_ds(1, False, True)\n",
    "small_ds = create_ds(1, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(4, 9, figsize=(16, 9))\n",
    "\n",
    "# for idx, (i, y) in enumerate(small_ds.take(36)):\n",
    "#     ax = axes.flat[idx]\n",
    "#     ax.set_axis_off()\n",
    "#     ax.imshow(tf.cast(i[0], tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT ONE BY ONE AND THE MERGE TOGETHER\n",
    "\n",
    "# for image, label in test.skip(1).take(1):\n",
    "#     # chop image\n",
    "#     image = image[0, 368 // 2 : -(368 // 2), 64 // 2 : -(64 // 2)]\n",
    "#     label = label[0, 368 // 2 : -(368 // 2), 64 // 2 : -(64 // 2)]\n",
    "\n",
    "#     for index in range(36):\n",
    "#         x, y = index // 9, index % 9\n",
    "#         img = image[448 * x : 448 * (1 + x), 448 * y : 448 * (1 + y)]\n",
    "\n",
    "#         img = model.predict(tf.expand_dims(img, 0))\n",
    "#         img = img[0]\n",
    "\n",
    "#         # concat horizontally\n",
    "#         if index == 0:\n",
    "#             prev_image = img\n",
    "#         else:\n",
    "#             prev_image = tf.concat([prev_image, img], axis=1)\n",
    "\n",
    "#     full_image = tf.concat(tf.split(prev_image, 4, axis=1), axis=0)\n",
    "#     print(tf.reduce_mean(JINDEX(full_image[tf.newaxis, ...], label[tf.newaxis, ...])))\n",
    "# for image, label in test.skip(1).take(1):\n",
    "#     # chop image\n",
    "#     image = image[0, 368 // 2 : -(368 // 2), 64 // 2 : -(64 // 2)]\n",
    "#     label = label[0, 368 // 2 : -(368 // 2), 64 // 2 : -(64 // 2)]\n",
    "\n",
    "#     image = model.predict(tf.expand_dims(image, 0))[0]\n",
    "#     assert len(image.shape) == 3\n",
    "#     print(tf.reduce_mean(JINDEX(image[tf.newaxis, ...], label[tf.newaxis, ...])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT CROPPED PICTURE\n",
    "\n",
    "# for image, label in test.skip(1).take(1):\n",
    "#     # chop image\n",
    "#     image = image[0, 368 // 2 : -(368 // 2), 64 // 2 : -(64 // 2)]\n",
    "#     label = label[0, 368 // 2 : -(368 // 2), 64 // 2 : -(64 // 2)]\n",
    "\n",
    "#     image = model.predict(tf.expand_dims(image, 0))[0]\n",
    "#     assert len(image.shape) == 3\n",
    "#     print(tf.reduce_mean(JINDEX(image[tf.newaxis, ...], label[tf.newaxis, ...])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 00:59:13.574596: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# PREDICT ON ORGINAL SIZE PICTURE\n",
    "\n",
    "for image, label in test.skip(5).take(1):\n",
    "    # chop image\n",
    "    image = tf.pad(image, [[0, 0], [8, 8], [0, 0], [0, 0]])\n",
    "    label = tf.pad(label, [[0, 0], [8, 8], [0, 0], [0, 0]])\n",
    "    # image = model.predict(image)\n",
    "    # print(tf.reduce_mean(JINDEX(image, label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save(label, 'label1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfZhVdb338fcHhEgh4obRhFHgLtR4kieJDl2KTwnakY5hQKZhD96eBEu9Tbw1Qi2vOnrqhKKGaeo5IHGmY1GH0jhCGoowmKmgCNEogynj+JDQQQG/9x97MWfPsGdmD8yaPbA+r+val3ut9dtrfdeo+7PX77ceFBGYmVl2dSh1AWZmVloOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgbULkr4t6TVJr5S6FrOscRBYHUlVkk4rot04SdUF5i+X9OV92O7RwBXAwIj4UCPbe0/StrzXL5NlsyXtTOa9KekxSR9Plk2TFJK+0WB91ZLGtbTOFuzPNEm/LzC/7u8r6Z6kttF5yz8iKfKml0va0XC/JZ2XN/3fDf82Bbb7vKQvFpj/NUmVyftBkh6S9Hryd1wj6cwm9m93g7puzVs+WtKSZD2vS1ol6cJk2bhkv29rsM7fS5pWxJ/XUuAgsPbgaKA2IrY20ebliOia9/r7vGU/jYiuQBnwe+A/JClZ9jrwDUnd0il9v7wOfLuZNtMb7ndEzN8zDUygwd+mwDruBS4oMP/8ZBnAL4HfAh8CDgcuBf7aRF2PN6hrOkASwg8DvwM+AvQE/jGpc4/twPmS+jWz79ZGHARWkKQOkq6V9KKkrZLuk9R9P9bXPVlHTbLOa5NtnEbuC6h38svynn3dRkTsJPfF9iFyX0AAzwGPA5cXUePHJL0iqWPevH+Q9HTyfrSkSkl/lfSqpO/va62Je4Ghkk7az/U051+BT0jqu2eGpIHAUOB+Sb2A/sCdEfFu8loREXsd1RThJuDeiPheRLwWOWsi4rN5bd4E7gG+tc97ZK3KQWCNmZa8Tgb+N9AVuLWJ9s25BeierOskcr9QL4yIpdT/VTttXzcg6X1JzZsj4rW8Rd8Evi7pfzX1+Yh4gtyv1VPyZn8OWJC8/yHww4j4APBhYNG+1pr4G3Aj8J39XE+TIqIaWEbuCGCP84Elyd+pFtgI/JukT0s6Yl+2I+lQ4ONARRHNvwN8RtKx+7Ita10OAmvMecD3I2JTRGwDrgamSDqkpStKfmFPAa6OiLcjogr4Z+p/MTWnd9LnvOeV/wvzs5LeBDYDI4F/yP9gRDxF7qjjqiK2cz8wNam7G3BmMg9gJ/ARSb0iYltErGxB/Y35EXC0pAmNLJ/TYL9v2Mft3Evy95bUgdy/33sBInfDsZOBKnL/Xv4i6RFJA5pY35gGdY0BepD7TvlLc8VExCvAHcD1+7g/1oocBNaY3sCLedMvAocARwC7gE4FPtOJ3JdlQ72SZQ3X16cF9bwcER/Me+X/Gl+UzDs8Ik6JiDUFPj8L+Mcifu0uAM5Jji7OAZ6MiD11fwk4Bnhe0mpJn2pkHUX/fSLiHeCG5FXIpQ32+5vN1N+Y/wCOTL6wxwGHAv+ZV0d1REyPiA8DfckdGd3XxPpWNqhrJfAG8B5wZJE1fQ84Q9LxLd8da00OAmvMy+S+EPY4mtwX3KvAS0AvSXUDk8ngbF/qf9nv8Rq5L8CG69vSyjU3KiKeJ/dleE0z7daR24cJ1O8WIiI2RMRUcoOp3wMqJB1WYDUvkfuVv2fAek+3yeEU/vv8BPggueBJRUT8jVyXzQXkjgwWRsS7jbTdDMwFBu/DNh4HPlNk+1rgX2g8BK2NOAisMfcDl0nqn3zh30ju7JxdEfES8ATwPUldk1/PV5L7st+ruyQidpPrT/+OpG7JoOXlwL+11c4krgMuJPel25QFwNeAE4F/3zNT0ucllUXEe+QGPCH3C7ihJ4AdwExJXZKw+C5QSYEgiIhd5AZOi+m62h/3ApPJfVHvOVsIST0kXafc6asdksHjL1Lg32URvgFMk3SlpJ7J+o+XtLCR9t8H/g746D5sy1qJg8Aacze5s00eAf5M7ottRt7yyeR+4W4k98v+VOCsiNjRyPpmkOtu2ETuFM8FyTbaTET8mdw+FfoVn+9+cgPaDzcYdB4PrE3O1f8hMCUi/rvAdt4BziLXBVNNbp97A5+Nxh8Acj+F+9ZvbXC+fqFur2I9ArwFVEfE6rz57wL9gKXkThl9FniH3MB7i0TEY+QG208BNkl6HZgHLGmk/V+BfwKaHMi3dMkPpjEzyzYfEZiZZZyDwMws4xwEZmYZ5yAwM8u4Fl8lWmq9evWKfv36lboMM7MDypo1a16LiLJCyw64IOjXrx+VlZWlLsPM7IAiqdDFjIC7hszMMs9BYGaWcQ4CM7OMO+DGCMzM9sXOnTuprq5mx47G7oJycOjSpQvl5eV06lToBriFOQjMLBOqq6vp1q0b/fr1I+/GsAeViKC2tpbq6mr69+9f9OfcNWRmmbBjxw569ux50IYAgCR69uzZ4qMeB4GZZcbBHAJ77Ms+OgjMzDLOQWBm1orOPPNM3nzzzSbbdO3ateD8adOmUVFRkUZZTfJgcRsZeWVTj3+tb81NF6RYiZmlISKICJYsKfgMnnYt1SMCSeMlrZe0UdLMAsuPlrRM0h8kPS3pzDTrMTNrzsyZM5k7d27d9OzZs/n2t7/NqaeeyogRIxgyZAi/+MUvAKiqquLYY4/lggsuYPDgwWzevJl+/frx2mu5B9t9+tOfZuTIkQwaNIh58+bV285ll13GoEGDOPXUU6mpqdmrjjVr1nDSSScxcuRIzjjjDP7yl0IPsGsdqQWBpI7kHoA9ARgITJU0sEGza4FFETEcmALcllY9ZmbFmDx5MosWLaqbXrRoEV/4whd44IEHePLJJ1m2bBlXXHEFe57uuGHDBr761a+ydu1a+vbtW29dd999N2vWrKGyspI5c+ZQW1sLwPbt2xk1ahRr167lpJNO4rrrrqv3uZ07dzJjxgwqKipYs2YNX/ziF7nmmmtS2+c0u4ZGAxsjYhNA8vDqicC6vDYBfCB53x14OcV6zMyaNXz4cLZu3crLL79MTU0NPXr04EMf+hCXXXYZjzzyCB06dGDLli28+uqrAPTt25cxY8YUXNecOXN44IEHANi8eTMbNmygZ8+edOjQgcmTJwPw+c9/nnPOOafe59avX8+zzz7L6aefDsDu3bs58sgj09rlVIOgD7A5b7oa+FiDNrOBhyTNIPdA8dNSrMfMrCjnnnsuFRUVvPLKK0yePJn58+dTU1PDmjVr6NSpE/369as7V/+www4ruI7ly5ezdOlSHn/8cQ499FDGjRvX6Pn9DU/5jAgGDRrE448/3ro71ohSnzU0FbgnIsqBM4F/lbRXTZIuklQpqbJQX5qZWWuaPHkyCxcupKKignPPPZe33nqLww8/nE6dOrFs2TJefLHROzrXeeutt+jRoweHHnoozz//PCtXrqxb9t5779WdHbRgwQI+8YlP1PvsscceS01NTV0Q7Ny5k7Vr17biHtaXZhBsAY7Kmy5P5uX7ErAIICIeB7oAvRquKCLmRcSoiBhVVlbwuQpmZq1m0KBBvP322/Tp04cjjzyS8847j8rKSoYMGcJ9993Hcccd1+w6xo8fz65du/joRz/KzJkz63UfHXbYYaxatYrBgwfz8MMPM2vWrHqf7dy5MxUVFVx11VUcf/zxDBs2jMcee6zV93OPNLuGVgMDJPUnFwBTgM81aPMScCpwj6SPkgsC/+Q3s5J75pln6t736tWr0W6aZ599tt50VVVV3ftf//rXBT+zbdu2gvPvueeeuvfDhg3jkUceKbLa/ZPaEUFE7AKmAw8Cz5E7O2itpOslnZ00uwL4iqQ/AvcD02LPULyZmbWJVC8oi4glwJIG82blvV8HjE2zBjMza1qpB4vNzKzEHARmZhnnIDAzyzgHgZlZxvnuo2aWSS25I3Axirlr8Jw5c7j99tsZMWIE8+fPb9Xt7w8HgZlZG7nttttYunQp5eXlpS6lHncNmZm1gYsvvphNmzYxYcIEunfvzvnnn8/HP/5xBgwYwJ133gnk7k80btw4Jk2axHHHHcd5551HW1xa5SMCM7M2cMcdd/Cb3/yGZcuWceutt/LAAw+wcuVKtm/fzvDhwznrrLMA+MMf/sDatWvp3bs3Y8eOZcWKFXvdi6i1+YjAzKwEJk6cyPvf/3569erFySefzKpVqwAYPXo05eXldOjQgWHDhtW7ZUVaHARmZiXQ8NbTe6bf97731c3r2LEju3btSr0WB4GZWQn84he/YMeOHdTW1rJ8+XJOOOGEktXiMQIzy6RiTvdM09ChQzn55JN57bXX+OY3v0nv3r154YUXSlKLg8DMrI3k9/cPHTqU++6rfy3DuHHjGDduXN30rbfe2iZ1uWvIzCzjfERgZtbGZs+eXeoS6vERgZlZxjkIzMwyLtUgkDRe0npJGyXNLLD8B5KeSl4vSHozzXrMzGxvqY0RSOoIzAVOB6qB1ZIWJ4+nBCAiLstrPwMYnlY9dnBpyZ0jS32aoFl7l+Zg8WhgY0RsApC0EJgIrGuk/VTgWynWY2ZW56Xrh7Tq+o6e9Uyrrq8pVVVVPPbYY3zuc59rlfWl2TXUB9icN12dzNuLpL5Af+DhRpZfJKlSUmVNTU2rF2pmdiCpqqpiwYIFrba+9jJYPAWoiIjdhRZGxLyIGBURo8rKytq4NDOz1lFVVcXgwYPrpm+++WZmz57NuHHjuOqqqxg9ejTHHHMMjz76KAC7d+/myiuv5IQTTmDo0KH86Ec/AmDmzJk8+uijDBs2jB/84Af7XVeaXUNbgKPypsuTeYVMAS5JsZaD1thbxrao/YoZK1KqxMz2x65du1i1ahVLlizhuuuuY+nSpdx11110796d1atX88477zB27Fg++clP8t3vfpebb76ZX/3qV62y7TSDYDUwQFJ/cgEwBdirQ0vScUAP4PEUazEza9fOOeccAEaOHFl3K4qHHnqIp59+moqKCgDeeustNmzYQOfOnVt126kFQUTskjQdeBDoCNwdEWslXQ9URsTipOkUYGG0xWN4zMxK6JBDDuG9996rm96xY0fd+z23n86/9XREcMstt3DGGWfUW8/y5ctbta5UxwgiYklEHBMRH46I7yTzZuWFABExOyL2usbAzOxgc8QRR7B161Zqa2t55513mu3aOeOMM7j99tvZuXMnAC+88ALbt2+nW7duvP32261Wl+81ZGaZ1Jane+7RqVMnZs2axejRo+nTpw/HHXdck+2//OUvU1VVxYgRI4gIysrK+PnPf87QoUPp2LEjxx9/PNOmTeOyyy5rcj3NcRCYmbWhSy+9lEsvvbTR5b169aobI+jQoQM33ngjN954417tHn644Nn2+6S9nD5qZmYl4iAwM8s4dw21Qy269L3HB9IrxMwywUcEZmYZ5yAwM8s4B4GZWcZ5jMDMMqml9+lqTjH38ZozZw633347I0aMYP78+a26/f3hIDAzayO33XYbS5cupby8vNSl1OOuITOzNnDxxRezadMmJkyYwA033MCFF17IkCFDGDp0KD/72c8A6Nq1K9dccw3HH388Y8aM4dVXX22T2hwEZmZt4I477qB3794sW7aMbdu20b17d5555hmefvppTjnlFAC2b9/OmDFj+OMf/8iJJ57InXfe2Sa1OQjMzNrY0qVLueSS/3kES48ePQDo3Lkzn/rUp4D6t6NOm4PAzKyd6NSpE5KA+rejTpuDwMysjZ1++unMnTu3bvqNN94oYTU+a8jMMqqUj2299tprueSSSxg8eDAdO3bkW9/6Vt0Tykoh1SCQNB74IbknlP04Ir5boM1ngdlAAH+MiL0eZ2lmdjDI7/O/995791q+bdu2uveTJk1i0qRJbVFWekEgqSMwFzgdqAZWS1ocEevy2gwArgbGRsQbkg5Pqx4zMysszSOC0cDGiNgEIGkhMBFYl9fmK8DciHgDICK2pliPFWHklfcV3XbNTRekWElptORq01J2LZi1pjQHi/sAm/Omq5N5+Y4BjpG0QtLKpCtpL5IuklQpqbKmpialcs3sYBcRpS4hdfuyj6U+a+gQYAAwDpgK3Cnpgw0bRcS8iBgVEaPKysrauEQzOxh06dKF2tragzoMIoLa2lq6dOnSos+l2TW0BTgqb7o8mZevGngiInYCf5b0ArlgWJ1iXWaWQeXl5VRXV3Ow9yp06dKlxfcySjMIVgMDJPUnFwBTgIZnBP2c3JHATyT1ItdVtCnFmswsozp16kT//v1LXUa7lFoQRMQuSdOBB8mdPnp3RKyVdD1QGRGLk2WflLQO2A1cGRG1adVk2eRHf5o1LdXrCCJiCbCkwbxZee8DuDx5mZlZCZR6sNjMzErMQWBmlnEOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcalGgSSxktaL2mjpJkFlk+TVCPpqeT15TTrMTOzvaX2hDJJHYG5wOnkHlK/WtLiiFjXoOlPI2J6WnWYmVnT0jwiGA1sjIhNEfEusBCYmOL2zMxsH6QZBH2AzXnT1cm8hj4j6WlJFZKOKrQiSRdJqpRUWVNTk0atZmaZVerB4l8C/SJiKPBb4N5CjSJiXkSMiohRZWVlbVqgmdnBLs0g2ALk/8IvT+bViYjaiHgnmfwxMDLFeszMrIA0g2A1MEBSf0mdgSnA4vwGko7MmzwbeC7FeszMrIDUzhqKiF2SpgMPAh2BuyNiraTrgcqIWAxcKulsYBfwOjAtrXrMzKyw1IIAICKWAEsazJuV9/5q4Oo0azAzs6aVerDYzMxKzEFgZpZxzQaBpCMk3SXp18n0QElfSr80MzNrC8WMEdwD/AS4Jpl+AfgpcFdKNbXIyCvvK7rtmpsuSLESM7MDUzFdQ70iYhHwHuTOBgJ2p1qVmZm1mWKOCLZL6gkEgKQxwFupVmV2kPARqx0IigmCy8ldCPZhSSuAMmBSqlWZmVmbaTYIIuJJSScBxwIC1kfEztQrMzOzNtFsEEhqeLw6QhIRUfwxr5mZtVvFdA2dkPe+C3Aq8CTgIDAzOwgU0zU0I39a0gfJPWTGzFrRS9cPKbrt0bOeSbESy5p9ubJ4O9C/tQsxM7PSKGaM4Jckp46SC46BwKI0izIza6glp+KCT8dtiWLGCG7Oe78LeDEiqlOqx8zM2lgxYwS/a4tCzMysNBoNAklv8z9dQvUWARERH0itKjMzazONDhZHRLeI+ECBV7diQ0DSeEnrJW2UNLOJdp+RFJJG7ctOmJnZviv6CWWSDid3HQEAEfFSM+07AnOB04FqYLWkxRGxrkG7bsDXgCdaULdZpo29ZWzRbVfMWJFiJXYwKOZ5BGdL2gD8GfgdUAX8uoh1jwY2RsSmiHiX3LUHEwu0uwH4HrCj2KLNzKz1FHMdwQ3AGOCFiOhP7srilUV8rg+wOW+6OplXR9II4KiI+M/iyjUzs9ZWTBDsjIhaoIOkDhGxDNjvvnxJHYDvA1cU0fYiSZWSKmtqavZ302ZmlqeYIHhTUlfgUWC+pB+Su7q4OVuAo/Kmy5N5e3QDBgPLJVWRO+pYXGjAOCLmRcSoiBhVVlZWxKbNzKxYxQTBMqA7uQHd3wB/Av6+iM+tBgZI6i+pMzCF3HMNAIiItyKiV0T0i4h+5Lqbzo6Iyhbug5mZ7YdiguAQ4CFgOblf8T9NuoqalDzScjrwIPAcsCgi1kq6XtLZ+16ymZm1pmKuLL4OuE7SUGAy8DtJ1RFxWhGfXQIsaTBvViNtxxVVsZmZtaqW3H10K/AKUAscnk45ZmbW1oq5juCrkpYD/wX0BL4SEUPTLszMzNpGMVcWHwV8PSKeSrsYO7D4QSpmB4dixgiubotCzMysNPblCWVmZnYQcRCYmWWcg8DMLOOKvg11FvlWv2aWBQ4CM2vRg+H9UPiDj7uGzMwyzkFgZpZxmeoaaskFUAD0KOrRzGZmBzQfEZiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcalGgSSxktaL2mjpJkFll8s6RlJT0n6vaSBadZjZmZ7Sy0IJHUE5gITgIHA1AJf9AsiYkhEDAP+Cfh+WvWYmVlhaR4RjAY2RsSmiHgXWAhMzG8QEX/NmzwMiBTrMTOzAtK8oKwPsDlvuhr4WMNGki4BLgc6A6cUWpGki4CLAI4++uhWL9TMLMtKPlgcEXMj4sPAVcC1jbSZFxGjImJUWVlZ2xZoZnaQSzMItpB73vEe5cm8xiwEPp1iPWZmVkCaQbAaGCCpv6TOwBRgcX4DSQPyJs8CNqRYj5mZFZDaGEFE7JI0HXgQ6AjcHRFrJV0PVEbEYmC6pNOAncAbwBfSqsfMzApL9e6jEbEEWNJg3qy8919Lc/tmZta8kg8Wm5lZaTkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8u4VC8oMzPLmpFX3ld02zU3XZBiJcVzEFibGHvL2KLbrpixIsVKzKwhdw2ZmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGOQjMzDIu1SCQNF7SekkbJc0ssPxySeskPS3pvyT1TbMeMzPbW2pBIKkjMBeYAAwEpkoa2KDZH4BRETEUqAD+Ka16zMyssDSPCEYDGyNiU0S8CywEJuY3iIhlEfG3ZHIlUJ5iPWZmVkCaQdAH2Jw3XZ3Ma8yXgF8XWiDpIkmVkiprampasUQzM2sXg8WSPg+MAm4qtDwi5kXEqIgYVVZW1rbFmZkd5NK819AW4Ki86fJkXj2STgOuAU6KiHdSrMfMzApI84hgNTBAUn9JnYEpwOL8BpKGAz8Czo6IrSnWYmZmjUgtCCJiFzAdeBB4DlgUEWslXS/p7KTZTUBX4N8lPSVpcSOrMzOzlKR6G+qIWAIsaTBvVt7709LcvpmZNa9dDBabmVnpOAjMzDLOQWBmlnEOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZl+rzCMzMDgRjbxlbdNsVM1akWElppHpEIGm8pPWSNkqaWWD5iZKelLRL0qQ0azEzs8JSCwJJHYG5wARgIDBV0sAGzV4CpgEL0qrDzMyalmbX0GhgY0RsApC0EJgIrNvTICKqkmXvpViHmZk1Ic2uoT7A5rzp6mSemZm1IwfEWUOSLpJUKamypqam1OWYmR1U0gyCLcBRedPlybwWi4h5ETEqIkaVlZW1SnFmZpaTZhCsBgZI6i+pMzAFWJzi9szMbB+kFgQRsQuYDjwIPAcsioi1kq6XdDaApBMkVQPnAj+StDateszMrLBULyiLiCXAkgbzZuW9X02uy8jMzErkgBgsNjOz9DgIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGpRoEksZLWi9po6SZBZa/T9JPk+VPSOqXZj1mZra31IJAUkdgLjABGAhMlTSwQbMvAW9ExEeAHwDfS6seMzMrLM1nFo8GNkbEJgBJC4GJwLq8NhOB2cn7CuBWSYqISLEuM7MD0thbxhbddsWMFUW3VVrfuZImAeMj4svJ9PnAxyJiel6bZ5M21cn0n5I2rzVY10XARcnkscD6Vi63F/Bas61Kz3W2rgOhzgOhRnCdrS2NOvtGRFmhBWkeEbSaiJgHzEtr/ZIqI2JUWutvLa6zdR0IdR4INYLrbG1tXWeag8VbgKPypsuTeQXbSDoE6A7UpliTmZk1kGYQrAYGSOovqTMwBVjcoM1i4AvJ+0nAwx4fMDNrW6l1DUXELknTgQeBjsDdEbFW0vVAZUQsBu4C/lXSRuB1cmFRCql1O7Uy19m6DoQ6D4QawXW2tjatM7XBYjMzOzD4ymIzs4xzEJiZZVymg6C5W2C0F5LulrQ1ue6iXZJ0lKRlktZJWivpa6WuqRBJXSStkvTHpM7rSl1TUyR1lPQHSb8qdS2NkVQl6RlJT0mqLHU9jZH0QUkVkp6X9Jykj5e6poYkHZv8Hfe8/irp66lvN6tjBMktMF4ATgeqyZ3lNDUi1jX5wRKQdCKwDbgvIgaXup5CJB0JHBkRT0rqBqwBPt3e/p6SBBwWEdskdQJ+D3wtIlaWuLSCJF0OjAI+EBGfKnU9hUiqAkY1vBC0vZF0L/BoRPw4OZPx0Ih4s9R1NSb5jtpC7iLbF9PcVpaPCOpugRER7wJ7boHR7kTEI+TOqmq3IuIvEfFk8v5t4DmgT2mr2lvkbEsmOyWvdvlrSFI5cBbw41LXcqCT1B04kdyZikTEu+05BBKnAn9KOwQg20HQB9icN11NO/ziOhAld5EdDjxR2koKS7pbngK2Ar+NiHZZJ/AvwDeA90pdSDMCeEjSmuR2MO1Rf6AG+EnS1fZjSYeVuqhmTAHub4sNZTkILAWSugI/A74eEX8tdT2FRMTuiBhG7mr30ZLaXXebpE8BWyNiTalrKcInImIEuTsNX5J0ZbY3hwAjgNsjYjiwHWjP44KdgbOBf2+L7WU5CIq5BYa1QNLn/jNgfkT8R6nraU7SNbAMGF/qWgoYC5yd9L8vBE6R9G+lLamwiNiS/HMr8AC5btf2phqozjv6qyAXDO3VBODJiHi1LTaW5SAo5hYYVqRkEPYu4LmI+H6p62mMpDJJH0zev5/cyQLPl7aqvUXE1RFRHhH9yP23+XBEfL7EZe1F0mHJyQEkXS2fBNrd2W0R8QqwWdKxyaxTqX9L/PZmKm3ULQQHyN1H09DYLTBKXFZBku4HxgG9JFUD34qIu0pb1V7GAucDzyT97wD/LyKWlLCmQo4E7k3OyOgALIqIdntq5gHgCOCB3O8ADgEWRMRvSltSo2YA85MffpuAC0tcT0FJoJ4O/J8222ZWTx81M7OcLHcNmZkZDgIzs8xzEJiZZZyDwMws4xwEZmYZ5yAwa4Skx1rYflx7vkuoWWMcBGaNiOK+7TMAAAFcSURBVIi/K3UNZm3BQWDWCEnbkn+Ok7Q8717285Mrqfc80+J5SU8C5+R99rDkORKrkpucTUzm/1DSrOT9GZIekeT/D62kMntlsVkLDQcGAS8DK4CxyUNY7gROATYCP81rfw2520J8MbmlxSpJS4GrgdWSHgXmAGdGRHu/u6gd5PxLxKw4qyKiOvnSfgroBxwH/DkiNkTuEv38m8J9EpiZ3G5jOdAFODoi/gZ8BfgtcGtE/KkN98GsIB8RmBXnnbz3u2n+/x0Bn4mI9QWWDQFqgd6tVJvZfvERgdm+ex7oJ+nDyfTUvGUPAjPyxhKGJ//sC1xBrqtpgqSPtWG9ZgU5CMz2UUTsAC4C/jMZLN6at/gGco/BfFrSWuCGvFt1/9+IeBn4EvBjSV3auHSzenz3UTOzjPMRgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ9/8BibOQY4WIBloAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# result for FPN\n",
    "result_fpn = [\n",
    "    0.46724114,\n",
    "    0.8189325,\n",
    "    0.6044639,\n",
    "    0.7086379,\n",
    "    0.45720816,\n",
    "    0.34781703,\n",
    "    0.42935002,\n",
    "    0.24366052,\n",
    "]\n",
    "\n",
    "# result for UNET\n",
    "result_unet = [\n",
    "    3.3367285e-01,\n",
    "    6.6008663e-01,\n",
    "    3.7074447e-01,\n",
    "    5.8561021e-01,\n",
    "    4.3773600e-01,\n",
    "    0.0000000e00,\n",
    "    7.9385777e-20,\n",
    "    1.1507687e-23,\n",
    "]\n",
    "\n",
    "result_fcn = [\n",
    "    0.3529532,\n",
    "    0.6904841,\n",
    "    0.23908164,\n",
    "    0.6038181,\n",
    "    0.4120346,\n",
    "    0.0,\n",
    "    0.11895951,\n",
    "    0.00323856,\n",
    "]\n",
    "\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    {\"fpn\": result_fpn, \"unet\": result_unet, \"fcn\": result_fcn},\n",
    ")\n",
    "\n",
    "results = results.melt()\n",
    "results[\"index\"] = [i % 8 for i in range(24)]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"IoU of FPN vs UNET VS FCN\")\n",
    "sb.barplot(data=results, x=\"index\", y=\"value\", hue=\"variable\") ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_492760/3826478158.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclasses_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "total_iou = 0\n",
    "classes_iou = tf.zeros([8])\n",
    "\n",
    "for d, y in test.take(1):\n",
    "    # add 8 padding horizontally\n",
    "    # d = sm.get_preprocessing('resnet50')(d)\n",
    "    d = tf.pad(d, [[0, 0], [8, 8], [0, 0], [0, 0]])\n",
    "    y = tf.pad(y, [[0, 0], [8, 8], [0, 0], [0, 0]])\n",
    "    # add thresholding to make image sharper\n",
    "    out_img = tf.cast(model(d, training=False), tf.float32)\n",
    "\n",
    "    fig, axs = plt.subplots(8, 4, figsize=(20, 20))\n",
    "    for i in range(8):\n",
    "        axs[i, 0].imshow(\n",
    "            (d[0, :, :, :] / 255.0)\n",
    "            * tf.repeat(y[0, :, :, i][..., tf.newaxis], 3, axis=-1)\n",
    "        )  # this is the AND of image and target mask\n",
    "        axs[i, 1].imshow(y[0, :, :, i])  # this is the target mask\n",
    "        axs[i, 2].imshow(\n",
    "            (d[0, :, :, :] / 255.0)\n",
    "            * tf.repeat(out_img[0, :, :, i][..., tf.newaxis], 3, axis=-1)\n",
    "        )  # this is the AND of image and predicted mask\n",
    "        axs[i, 3].imshow(out_img[0, :, :, i])  # this is the predicted mask\n",
    "\n",
    "    num += 1\n",
    "    classes_iou += JINDEX(y, out_img)\n",
    "print(classes_iou / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_save(images, name):\n",
    "    for idx in range(images[0].shape[-1]):\n",
    "        byte = tf.image.encode_jpeg(\n",
    "            tf.image.convert_image_dtype(images[0, ..., idx][..., tf.newaxis], tf.uint8)\n",
    "        )\n",
    "        tf.io.write_file(f\"{name}_{idx}_image.jpeg\", byte)\n",
    "\n",
    "\n",
    "def to_save_mask(images, mask, name):\n",
    "    for idx in range(mask[0].shape[-1]):\n",
    "        thismask = tf.repeat(mask[0, ..., idx][..., tf.newaxis], 3, axis=-1)\n",
    "        image = images[0] * thismask\n",
    "        byte = tf.image.encode_jpeg(\n",
    "            tf.image.convert_image_dtype(image / 255.0, tf.uint8)\n",
    "        )\n",
    "        tf.io.write_file(f\"{name}_{idx}_image.jpeg\", byte)\n",
    "\n",
    "\n",
    "# to_save_mask(d, y, \"target\")\n",
    "# to_save_mask(d, out_img, \"mask\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0beed873570eadf18b27de988f74387134654fe26ad0c1ed6b53170102862c4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tf21': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

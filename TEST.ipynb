{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVIDDataset(Dataset):\n",
    "    def __init__(self, path, is_train=True, is_4k_image=False):\n",
    "        self.is_4k_image = is_4k_image\n",
    "        directory = Path(path)\n",
    "        if is_train:\n",
    "            self.images = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_train/**/Images/*.png\")\n",
    "            ]\n",
    "            self.labels = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_train/**/Labels/*.png\")\n",
    "            ]\n",
    "        else:\n",
    "            self.images = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_val/**/Images/*.png\")\n",
    "            ]\n",
    "            self.labels = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_val/**/Labels/*.png\")\n",
    "            ]\n",
    "\n",
    "        if len(self.images) is not len(self.labels):\n",
    "            print(\"Number of images & label are not the same.\")\n",
    "            return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_image(image_path):\n",
    "        return read_image(image_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_image(image):\n",
    "        resizer = Resize([2160, 3840], antialias=\"True\")\n",
    "        return resizer(image)\n",
    "\n",
    "    @staticmethod\n",
    "    def label_0and1(label):\n",
    "        return label.type(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def image_0and1(image):\n",
    "        return (image / 255).type(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_label(label):\n",
    "        labels = []\n",
    "        labels.append((label[0] == 0) & (label[1] == 0) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 0) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 64) & (label[2] == 128))\n",
    "        labels.append((label[0] == 0) & (label[1] == 128) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 128) & (label[2] == 0))\n",
    "        labels.append((label[0] == 64) & (label[1] == 0) & (label[2] == 128))\n",
    "        labels.append((label[0] == 192) & (label[1] == 0) & (label[2] == 192))\n",
    "        labels.append((label[0] == 64) & (label[1] == 64) & (label[2] == 0))\n",
    "        return labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.decode_image(self.images[index])\n",
    "        image = self.resize_image(image)\n",
    "        image = self.image_0and1(image)\n",
    "\n",
    "        label = self.decode_image(self.labels[index])\n",
    "        label = self.resize_image(label)\n",
    "        label = self.label_0and1(label)\n",
    "        label = self.mask_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torch.utils.data.dataset.TensorDataset'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/babi/Programs/featureanalysis/test.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/babi/Programs/featureanalysis/test.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_data \u001b[39m=\u001b[39m UAVIDDataset(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/babi/Programs/featureanalysis/test.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/uavid_v1.5_official_release_image/\u001b[39m\u001b[39m\"\u001b[39m, is_train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/babi/Programs/featureanalysis/test.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/babi/Programs/featureanalysis/test.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(training_data, batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/babi/Programs/featureanalysis/test.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_feature, train_label \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_dataloader))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/babi/Programs/featureanalysis/test.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(train_feature\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/featureanalysis/test.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# figure, axes = plt.subplots(8, 1, figsize=(16, 9))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/featureanalysis/test.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# for i in range(8):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/featureanalysis/test.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#     axes[i].set_axis_off()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/featureanalysis/test.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#     axes[i].imshow(torch.permute(train_label, [0, 2, 3, 1]).numpy()[0,..., i], cmap='gray', vmin=0, vmax=1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:150\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m             \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    148\u001b[0m             \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[0;32m--> 150\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torch.utils.data.dataset.TensorDataset'>"
     ]
    }
   ],
   "source": [
    "training_data = UAVIDDataset(\n",
    "    path=\"data/uavid_v1.5_official_release_image/\", is_train=False\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=4, shuffle=True)\n",
    "\n",
    "train_feature, train_label = next(iter(train_dataloader))\n",
    "\n",
    "print(train_feature.shape)\n",
    "\n",
    "# figure, axes = plt.subplots(8, 1, figsize=(16, 9))\n",
    "# for i in range(8):\n",
    "#     axes[i].set_axis_off()\n",
    "#     axes[i].imshow(torch.permute(train_label, [0, 2, 3, 1]).numpy()[0,..., i], cmap='gray', vmin=0, vmax=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import ResNet50_Weights, resnet50, resnet34, ResNet34_Weights\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.transforms import RandomCrop, Resize, transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "import torch.autograd.profiler as profiler\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256]) tensor(-0.6673) tensor(1.0352)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MultiNet(nn.Module):\n",
    "    def __init__(self, numberClass):\n",
    "        super().__init__()\n",
    "        _resnet34 = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = create_feature_extractor(\n",
    "            _resnet34,\n",
    "            {\n",
    "                # \"relu\": \"feat1\",\n",
    "                \"layer1\": \"feat2\",\n",
    "                \"layer2\": \"feat3\",\n",
    "                \"layer3\": \"feat4\",\n",
    "                \"layer4\": \"feat5\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_prediction = self.backbone(torch.rand([1, 3, 256, 256])).values()\n",
    "            backbone_dimensions = [output.size(1) for output in outputs_prediction]\n",
    "\n",
    "        # Freeze backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.upsampling_2x_bilinear = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        self.upsampling_4x_bilinear = nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "        self.upsampling_8x_bilinear = nn.UpsamplingBilinear2d(scale_factor=8)\n",
    "        self.conv5_1x1 = nn.Conv2d(\n",
    "            in_channels=backbone_dimensions[-1],\n",
    "            out_channels=256,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.conv5_3x3_1 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv5_3x3_2 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=numberClass,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv4_1x1 = nn.Conv2d(\n",
    "            in_channels=backbone_dimensions[-2],\n",
    "            out_channels=256,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.conv4_3x3_1 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv4_3x3_2 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=numberClass,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv3_1x1 = nn.Conv2d(\n",
    "            in_channels=backbone_dimensions[-3],\n",
    "            out_channels=256,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.conv3_3x3_1 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv3_3x3_2 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=numberClass,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv2_1x1 = nn.Conv2d(\n",
    "            in_channels=backbone_dimensions[-4],\n",
    "            out_channels=256,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.conv2_3x3_1 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv2_3x3_2 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=numberClass,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_output = self.backbone(x)\n",
    "        feat2, feat3, feat4, feat5 = (\n",
    "            backbone_output[\"feat2\"],\n",
    "            backbone_output[\"feat3\"],\n",
    "            backbone_output[\"feat4\"],\n",
    "            backbone_output[\"feat5\"],\n",
    "        )\n",
    "\n",
    "        conv5_mid = self.conv5_1x1(feat5).relu()\n",
    "        conv5_prediction = self.conv5_3x3_1(conv5_mid).relu()\n",
    "        conv5_prediction = self.conv5_3x3_2(conv5_prediction)\n",
    "\n",
    "        conv4_lateral = self.conv4_1x1(feat4).relu()\n",
    "        conv4_mid = conv4_lateral + self.upsampling_2x_bilinear(conv5_mid)\n",
    "        conv4_prediction = self.conv4_3x3_1(conv4_mid).relu()\n",
    "        conv4_prediction = self.conv4_3x3_2(conv4_prediction)\n",
    "\n",
    "        conv3_lateral = self.conv3_1x1(feat3).relu()\n",
    "        conv3_mid = conv3_lateral + self.upsampling_2x_bilinear(conv4_mid)\n",
    "        conv3_prediction = self.conv3_3x3_1(conv3_mid).relu()\n",
    "        conv3_prediction = self.conv3_3x3_2(conv3_prediction)\n",
    "\n",
    "        conv2_lateral = self.conv2_1x1(feat2).relu()\n",
    "        conv2_mid = conv2_lateral + self.upsampling_2x_bilinear(conv3_mid)\n",
    "        conv2_prediction = self.conv2_3x3_1(conv2_mid).relu()\n",
    "        conv2_prediction = self.conv2_3x3_2(conv2_prediction)\n",
    "\n",
    "        final_prediction_5 = self.upsampling_8x_bilinear(conv5_prediction)\n",
    "        final_prediction_4 = self.upsampling_4x_bilinear(conv4_prediction)\n",
    "        final_prediction_3 = self.upsampling_2x_bilinear(conv3_prediction)\n",
    "        final_prediction_2 = conv2_prediction\n",
    "\n",
    "        return self.upsampling_4x_bilinear(\n",
    "            torch.sum(\n",
    "                torch.concatenate(\n",
    "                    [\n",
    "                        final_prediction_5,\n",
    "                        final_prediction_4,\n",
    "                        final_prediction_3,\n",
    "                        final_prediction_2,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                ),\n",
    "                dim=1,\n",
    "                keepdim=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "model = MultiNet(8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(torch.rand([1, 3, 256, 256]))\n",
    "    print(output.size(), output.min(), output.max())\n",
    "    plt.imshow(torch.argmax(output[0], dim=0) / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVIDDataset4K(Dataset):\n",
    "    def __init__(self, path, is_train=True):\n",
    "        directory = Path(path)\n",
    "        if is_train:\n",
    "            self.images = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_train/**/Images/*.png\")\n",
    "            ]\n",
    "            self.labels = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_train/**/Labels/*.png\")\n",
    "            ]\n",
    "        else:\n",
    "            self.images = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_val/**/Images/*.png\")\n",
    "            ]\n",
    "            self.labels = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_val/**/Labels/*.png\")\n",
    "            ]\n",
    "\n",
    "        if len(self.images) is not len(self.labels):\n",
    "            print(\"Number of images & label are not the same.\")\n",
    "            return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_image(image_path):\n",
    "        return read_image(image_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_image(image):\n",
    "        resizer = Resize([2160, 3840], antialias=\"True\")\n",
    "        return resizer(image)\n",
    "\n",
    "    @staticmethod\n",
    "    def label_0and1(label):\n",
    "        return label.type(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def image_0and1(image):\n",
    "        return (image / 255).type(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_label(label):\n",
    "        labels = []\n",
    "        labels.append((label[0] == 0) & (label[1] == 0) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 0) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 64) & (label[2] == 128))\n",
    "        labels.append((label[0] == 0) & (label[1] == 128) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 128) & (label[2] == 0))\n",
    "        labels.append((label[0] == 64) & (label[1] == 0) & (label[2] == 128))\n",
    "        labels.append((label[0] == 192) & (label[1] == 0) & (label[2] == 192))\n",
    "        labels.append((label[0] == 64) & (label[1] == 64) & (label[2] == 0))\n",
    "        return torch.stack(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.decode_image(self.images[index])\n",
    "        image = self.resize_image(image)\n",
    "        image = self.image_0and1(image)\n",
    "\n",
    "        label = self.decode_image(self.labels[index])\n",
    "        label = self.resize_image(label)\n",
    "        label = self.label_0and1(label)\n",
    "        label = self.mask_label(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVIDDataset(Dataset):\n",
    "    def __init__(self, path, is_train=True):\n",
    "        directory = Path(path)\n",
    "        if is_train:\n",
    "            self.images = [\n",
    "                str(x.absolute()) for x in directory.glob(\"train/image/*.png\")\n",
    "            ]\n",
    "            self.labels = [\n",
    "                str(x.absolute()) for x in directory.glob(\"train/label/*.png\")\n",
    "            ]\n",
    "        else:\n",
    "            self.images = [\n",
    "                str(x.absolute()) for x in directory.glob(\"test/image/*.png\")\n",
    "            ]\n",
    "            self.labels = [\n",
    "                str(x.absolute()) for x in directory.glob(\"test/label/*.png\")\n",
    "            ]\n",
    "\n",
    "        if len(self.images) != len(self.labels):\n",
    "            print(\"Number of images & label are not the same.\")\n",
    "            return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_image(image_path):\n",
    "        return read_image(image_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_image(image):\n",
    "        resizer = Resize([2160, 3840], antialias=\"True\")\n",
    "        return resizer(image)\n",
    "\n",
    "    @staticmethod\n",
    "    def label_0and1(label):\n",
    "        return label.type(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def image_0and1(image):\n",
    "        return (image / 255).type(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_label(label):\n",
    "        labels = []\n",
    "        labels.append((label[0] == 0) & (label[1] == 0) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 0) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 64) & (label[2] == 128))\n",
    "        labels.append((label[0] == 0) & (label[1] == 128) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 128) & (label[2] == 0))\n",
    "        labels.append((label[0] == 64) & (label[1] == 0) & (label[2] == 128))\n",
    "        labels.append((label[0] == 192) & (label[1] == 0) & (label[2] == 192))\n",
    "        labels.append((label[0] == 64) & (label[1] == 64) & (label[2] == 0))\n",
    "        return torch.stack(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.decode_image(self.images[index])\n",
    "        i, j, h, w = RandomCrop.get_params(image, (256, 256))\n",
    "        image = self.image_0and1(image)\n",
    "        label = self.decode_image(self.labels[index])\n",
    "        label = self.mask_label(label)\n",
    "        label = self.label_0and1(label)\n",
    "\n",
    "        # Crop image and label\n",
    "        image = crop(image, i, j, h, w)\n",
    "        label = crop(label, i, j, h, w)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labels = [\n",
    "    \"background\",\n",
    "    \"building\",\n",
    "    \"road\",\n",
    "    \"tree\",\n",
    "    \"vegetation\",\n",
    "    \"moving_car\",\n",
    "    \"stationary_car\",\n",
    "    \"human\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_channels(image: torch.Tensor, colors: np.ndarray, is_predict: bool):\n",
    "    _, _, h, w = image.shape\n",
    "    output_image = np.zeros([h, w, 3], dtype=np.uint8)\n",
    "    for i in range(colors.shape[0]):\n",
    "        if is_predict:\n",
    "            mask = image[0, i] > 0.5\n",
    "        else:\n",
    "            mask = image[0, i] == 1\n",
    "        output_image[mask] = colors[i]\n",
    "    return output_image\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    input_image: torch.Tensor,\n",
    "    grouth_truth: torch.Tensor,\n",
    "    predicted: torch.Tensor,\n",
    "):\n",
    "    colors = np.array(\n",
    "        [\n",
    "            [0, 0, 0],\n",
    "            [128, 0, 0],\n",
    "            [128, 64, 128],\n",
    "            [0, 128, 0],\n",
    "            [128, 128, 0],\n",
    "            [64, 0, 128],\n",
    "            [192, 0, 192],\n",
    "            [0, 0, 128],\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    )\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 9), dpi=200)\n",
    "    legend_patches = [\n",
    "        patches.Patch(\n",
    "            color=np.concatenate([color / 255, [1]]),\n",
    "            label=dataset_labels[idx],\n",
    "        )\n",
    "        for idx, color in enumerate(colors)\n",
    "    ]\n",
    "    fig.legend(handles=legend_patches, bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    grouth_truth_image = combine_channels(grouth_truth, colors, False)\n",
    "    predicted_image = combine_channels(predicted, colors, True)\n",
    "    input_image = torch.permute(input_image[0], [1, 2, 0])\n",
    "\n",
    "    axes[0].set_axis_off()\n",
    "    axes[1].set_axis_off()\n",
    "    axes[2].set_axis_off()\n",
    "\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[1].set_title('Grouth Truth Image')\n",
    "    axes[2].set_title('Predicted Image')\n",
    "\n",
    "    axes[0].imshow(input_image)\n",
    "    axes[1].imshow(grouth_truth_image)\n",
    "    axes[2].imshow(predicted_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = UAVIDDataset4K(path=\"data/uavid_v1.5_official_release_image\", is_train=True)\n",
    "training_data = UAVIDDataset(\n",
    "    path=\"/Users/babi/Programs/high_performance_analysis_system/data/processed_dataset/\",\n",
    "    is_train=True,\n",
    ")\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True)\n",
    "train_feature, train_label = next(iter(train_dataloader))\n",
    "\n",
    "visualize(train_feature, train_label, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNETNetwork(nn.Module):\n",
    "    def __init__(self, numberClass):\n",
    "        super().__init__()\n",
    "        _resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = create_feature_extractor(\n",
    "            _resnet50,\n",
    "            {\n",
    "                \"layer1\": \"feat2\",\n",
    "                \"layer2\": \"feat3\",\n",
    "                \"layer3\": \"feat4\",\n",
    "                \"layer4\": \"feat5\",\n",
    "            },\n",
    "        )\n",
    "        self.upsampling_2x_bilinear = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=2048, out_channels=1024, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv6 = nn.Conv2d(\n",
    "            in_channels=1024, out_channels=512, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv7 = nn.Conv2d(\n",
    "            in_channels=512, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv8 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=128, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.convfinal = nn.Conv2d(\n",
    "            in_channels=128, out_channels=numberClass, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # with profiler.record_function(\"BACKBONE\"):\n",
    "        backbone_output = self.backbone(x)\n",
    "\n",
    "        feat2, feat3, feat4, feat5 = (\n",
    "            backbone_output[\"feat2\"],\n",
    "            backbone_output[\"feat3\"],\n",
    "            backbone_output[\"feat4\"],\n",
    "            backbone_output[\"feat5\"],\n",
    "        )\n",
    "\n",
    "        # with profiler.record_function(\"FRONTEND\"):\n",
    "        feat4to6 = self.upsampling_2x_bilinear(self.conv5(feat5).relu())\n",
    "        feat3to7 = self.upsampling_2x_bilinear(self.conv6(feat4 + feat4to6).relu())\n",
    "        feat2to8 = self.upsampling_2x_bilinear(self.conv7(feat3 + feat3to7).relu())\n",
    "        featout = self.upsampling_2x_bilinear(self.conv8(feat2 + feat2to8).relu())\n",
    "\n",
    "        return self.upsampling_2x_bilinear(self.convfinal(featout))\n",
    "\n",
    "\n",
    "model = UNETNetwork(8).to('mps')\n",
    "thisinput = torch.rand(1, 3, 256, 256, device='mps')\n",
    "model(thisinput)\n",
    "\n",
    "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
    "    out = model(thisinput)\n",
    "\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPNNetwork(nn.Module):\n",
    "    def __init__(self, numberClass):\n",
    "        super().__init__()\n",
    "        _resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = create_feature_extractor(\n",
    "            _resnet50,\n",
    "            {\n",
    "                \"relu\": \"feat1\",\n",
    "                \"layer1\": \"feat2\",\n",
    "                \"layer2\": \"feat3\",\n",
    "                \"layer3\": \"feat4\",\n",
    "                \"layer4\": \"feat5\",\n",
    "            },\n",
    "        )\n",
    "        self.upsampling_2x_bilinear = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        self.upsampling_4x_bilinear = nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "        self.upsampling_8x_bilinear = nn.UpsamplingBilinear2d(scale_factor=8)\n",
    "        self.upsampling_16x_bilinear = nn.UpsamplingBilinear2d(scale_factor=16)\n",
    "        # self.conv5 = nn.Conv2d(\n",
    "        #     in_channels=2048, out_channels=64, kernel_size=3, padding=1\n",
    "        # )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=2048, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv6 = nn.Conv2d(\n",
    "            in_channels=1024, out_channels=512, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv7 = nn.Conv2d(\n",
    "            in_channels=512, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv8 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=128, kernel_size=3, padding=1\n",
    "        )\n",
    "        # self.convfinal = nn.Conv2d(\n",
    "        #     in_channels=64, out_channels=numberClass, kernel_size=1\n",
    "        # )\n",
    "        self.convfinal = nn.Conv2d(\n",
    "            in_channels=256, out_channels=numberClass, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_output = self.backbone(x)\n",
    "        feat1, feat2, feat3, feat4, feat5 = (\n",
    "            backbone_output[\"feat1\"],\n",
    "            backbone_output[\"feat2\"],\n",
    "            backbone_output[\"feat3\"],\n",
    "            backbone_output[\"feat4\"],\n",
    "            backbone_output[\"feat5\"],\n",
    "        )\n",
    "\n",
    "        # print(feat1.size())\n",
    "        # print(feat2.size())\n",
    "        # print(feat3.size())\n",
    "        # print(feat4.size())\n",
    "        # print(feat5.size())\n",
    "\n",
    "        # return self.upsampling_2x_bilinear(\n",
    "        #     self.convfinal(self.upsampling_16x_bilinear(self.conv5(feat5)) + feat1)\n",
    "        # )\n",
    "\n",
    "        return self.upsampling_4x_bilinear(\n",
    "            self.convfinal(self.upsampling_8x_bilinear(self.conv5(feat5)) + feat2)\n",
    "        )\n",
    "\n",
    "        # feat4to6 = self.upsampling_2x_bilinear(self.conv5(feat5).relu())\n",
    "        # feat3to7 = self.upsampling_2x_bilinear(self.conv6(feat4 + feat4to6).relu())\n",
    "        # feat2to8 = self.upsampling_2x_bilinear(self.conv7(feat3 + feat3to7).relu())\n",
    "        # featout = self.upsampling_2x_bilinear(self.conv8(feat2 + feat2to8).relu())\n",
    "        # return self.upsampling_2x_bilinear(self.convfinal(featout))\n",
    "\n",
    "\n",
    "model = FPNNetwork(8)\n",
    "with torch.no_grad():\n",
    "    print(model(torch.rand(1, 3, 256, 256)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_index(\n",
    "    pred: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    epsilon=1e-9,\n",
    "):\n",
    "    pred_flat = pred.flatten()\n",
    "    target_flat = target.flatten()\n",
    "    nominator = 2 * torch.matmul(pred_flat, target_flat)\n",
    "    denominator = torch.sum(pred_flat) + torch.sum(target_flat)\n",
    "    return (nominator + epsilon) / (denominator + epsilon)\n",
    "\n",
    "\n",
    "def dice_index_per_channel(\n",
    "    pred: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    epsilon=1e-9,\n",
    "):\n",
    "    pred_flat = pred.permute([1, 0, 2, 3]).flatten(1)\n",
    "    label_flat = target.permute([1, 0, 2, 3]).flatten(1)\n",
    "    nominator = 2 * torch.sum(pred_flat * label_flat, dim=1)\n",
    "    denominator = torch.sum(pred_flat, dim=1) + torch.sum(label_flat, dim=1)\n",
    "    return (nominator + epsilon) / (denominator + epsilon)\n",
    "\n",
    "\n",
    "def total_loss(pred: torch.Tensor, target: torch.Tensor):\n",
    "    crossentropy_loss = torch.nn.functional.cross_entropy(pred, target)\n",
    "    dice_loss = 1 - dice_index(pred.softmax(1), target)\n",
    "    return crossentropy_loss + dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = UAVIDDataset(\n",
    "    path=\"/Users/babi/Programs/high_performance_analysis_system/data/processed_dataset/\",\n",
    "    is_train=True,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "model = FPNNetwork(numberClass=8).to(\"mps\")\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]).to(\"mps\")\n",
    "timestamp = datetime.datetime.now().strftime(r\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(\"data/training/train_{}\".format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('data/savedmodel/49_model.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_dice = np.zeros(8)\n",
    "    for idx, data in enumerate(tqdm(train_dataloader)):\n",
    "        inputs: torch.Tensor\n",
    "        labels: torch.Tensor\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = inputs.to(\"mps\")\n",
    "        labels = labels.to(\"mps\")\n",
    "\n",
    "        inputs = normalize(inputs)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = total_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        dice_per_channel = dice_index_per_channel(outputs.softmax(1), labels)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_dice += dice_per_channel.tolist()\n",
    "\n",
    "        if idx % 100 == 99:\n",
    "            current_training_sample = epoch * len(train_dataloader) + idx + 1\n",
    "            current_dice = running_dice / 100\n",
    "            writer.add_scalar(\"Loss/train\", running_loss / 100, current_training_sample)\n",
    "            writer.add_scalars(\n",
    "                \"dice/train\",\n",
    "                {name: current_dice[i] for i, name in enumerate(dataset_labels)},\n",
    "                current_training_sample,\n",
    "            )\n",
    "            print(f\"Loss: {running_loss / 100}\")\n",
    "            running_loss = 0.0\n",
    "            running_dice *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_data = UAVIDDataset(\n",
    "    path=\"/Users/babi/Programs/high_performance_analysis_system/data/processed_dataset/\",\n",
    "    is_train=False,\n",
    ")\n",
    "test_dataloader = DataLoader(training_data, batch_size=1, shuffle=False)\n",
    "test_feature, test_label = next(iter(test_dataloader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(normalize(test_feature.to(\"mps\")))\n",
    "    outputs = outputs.to(\"cpu\").softmax(axis=1)\n",
    "    visualize(test_feature, test_label, outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

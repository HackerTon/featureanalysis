{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from numpy.typing import NDArray\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.models import (\n",
    "    EfficientNet_V2_S_Weights,\n",
    "    MobileNet_V3_Large_Weights,\n",
    "    MobileNet_V3_Small_Weights,\n",
    "    ResNet50_Weights,\n",
    "    efficientnet_v2_l,\n",
    "    mobilenet_v3_large,\n",
    "    mobilenet_v3_small,\n",
    "    resnet50,\n",
    ")\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.transforms import RandomCrop, Resize, transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "class TextOCRDataset(Dataset):\n",
    "    def __init__(self, directory, is_train=True):\n",
    "        if directory == None:\n",
    "            print(\"Directory is none\")\n",
    "            return\n",
    "        self.directory = Path(directory)\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        if is_train:\n",
    "            self.decode(\n",
    "                file_path=str(self.directory.joinpath(\"TextOCR_0.1_train.json\"))\n",
    "            )\n",
    "        else:\n",
    "            self.decode(file_path=str(self.directory.joinpath(\"TextOCR_0.1_val.json\")))\n",
    "\n",
    "    def decode(self, file_path: str):\n",
    "        validation_label = json.load(open(file_path))\n",
    "        for image_id, image in validation_label[\"imgToAnns\"].items():\n",
    "            bounding_box_each_image = []\n",
    "            for annotation in image:\n",
    "                annot = validation_label[\"anns\"][f\"{annotation}\"]\n",
    "                bounding_box = annot[\"bbox\"]\n",
    "                x1, y1 = int(bounding_box[0]), int(bounding_box[1])\n",
    "                x2, y2 = x1 + int(bounding_box[2]), y1 + int(bounding_box[3])\n",
    "                bounding_box_each_image.append([x1, y1, x2, y2])\n",
    "            self.images.append(image_id)\n",
    "            self.labels.append(bounding_box_each_image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.decode_image(\n",
    "            str(self.directory.joinpath(\"train_images\", f\"{self.images[index]}.jpg\"))\n",
    "        )\n",
    "        mask = torch.zeros([1, image.size(1), image.size(2)])\n",
    "        for bounding_box in self.labels[index]:\n",
    "            x1, y1, x2, y2 = bounding_box\n",
    "            _, w, h =mask[..., y1:y2, x1:x2].size()\n",
    "            mask[..., y1:y2, x1:x2] = torch.tensor([255]).repeat(1, w, h)\n",
    "\n",
    "        i, j, h, w = RandomCrop.get_params(image, (256, 256))\n",
    "        # Crop image and label\n",
    "        image = crop(image, i, j, h, w)\n",
    "        mask = crop(mask, i, j, h, w)\n",
    "\n",
    "        masked = torch.cat([mask, torch.abs(1 - mask)])\n",
    "        return image.to(torch.float32) / 255, masked.to(torch.float32) / 255\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_image(image_path):\n",
    "        return read_image(image_path, ImageReadMode.RGB)\n",
    "\n",
    "\n",
    "dataset = TextOCRDataset(directory=\"../data/dataset/\", is_train=True)\n",
    "train_dataloader = DataLoader(dataset, batch_size=5)\n",
    "for idx, sample in enumerate(train_dataloader):\n",
    "    print(sample[1].size())\n",
    "    \n",
    "    if idx == 0:\n",
    "        break\n",
    "\n",
    "# for i, sample in enumerate(dataset):\n",
    "#     # if sample[0].size(0) == 1:\n",
    "#     #     break\n",
    "#     figures, axes = plt.subplots(1, 3)\n",
    "#     axes[0].imshow(torch.permute(sample[0], dims=[1, 2, 0]))\n",
    "#     axes[1].imshow(sample[1][0], cmap='gray')\n",
    "#     axes[2].imshow(sample[1][1], cmap='gray')\n",
    "\n",
    "\n",
    "#     if i == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1][1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_channels(image: torch.Tensor, colors: np.ndarray, is_predict: bool):\n",
    "    _, _, h, w = image.shape\n",
    "    output_image = np.zeros([h, w, 3], dtype=np.uint8)\n",
    "    for i in range(colors.shape[0]):\n",
    "        if is_predict:\n",
    "            mask = image[0, i] > 0.5\n",
    "        else:\n",
    "            mask = image[0, i] == 1\n",
    "        output_image[mask] = colors[i]\n",
    "    return output_image\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    input_image: torch.Tensor,\n",
    "    grouth_truth: torch.Tensor,\n",
    "    predicted: torch.Tensor,\n",
    "):\n",
    "    colors = np.array(\n",
    "        [\n",
    "            [0, 0, 0],\n",
    "            [128, 0, 0],\n",
    "            [128, 64, 128],\n",
    "            [0, 128, 0],\n",
    "            [128, 128, 0],\n",
    "            [64, 0, 128],\n",
    "            [192, 0, 192],\n",
    "            [0, 0, 128],\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    )\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 9), dpi=200)\n",
    "    legend_patches = [\n",
    "        patches.Patch(\n",
    "            color=np.concatenate([color / 255, [1]]),\n",
    "            label=UAVIDDataset.dataset_labels[idx],\n",
    "        )\n",
    "        for idx, color in enumerate(colors)\n",
    "    ]\n",
    "    fig.legend(handles=legend_patches, bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    grouth_truth_image = combine_channels(grouth_truth, colors, False)\n",
    "    predicted_image = combine_channels(predicted, colors, True)\n",
    "    input_image = torch.permute(input_image[0], [1, 2, 0])\n",
    "\n",
    "    axes[0].set_axis_off()\n",
    "    axes[1].set_axis_off()\n",
    "    axes[2].set_axis_off()\n",
    "\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "    axes[1].set_title(\"Grouth Truth Image\")\n",
    "    axes[2].set_title(\"Predicted Image\")\n",
    "\n",
    "    axes[0].imshow(input_image)\n",
    "    axes[1].imshow(grouth_truth_image)\n",
    "    axes[2].imshow(predicted_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training_data = UAVIDDataset4K(path=\"data/uavid_v1.5_official_release_image\", is_train=True)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m training_data \u001b[38;5;241m=\u001b[39m UAVIDDataset(\n\u001b[1;32m      3\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/processed_dataset/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(training_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m train_feature, train_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_dataloader))\n\u001b[1;32m      8\u001b[0m visualize(train_feature, train_label, train_label)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:349\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 349\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m RandomSampler(dataset, generator\u001b[38;5;241m=\u001b[39mgenerator)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/sampler.py:140\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# training_data = UAVIDDataset4K(path=\"data/uavid_v1.5_official_release_image\", is_train=True)\n",
    "training_data = UAVIDDataset(\n",
    "    path=\"data/processed_dataset/\",\n",
    "    is_train=True,\n",
    ")\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True)\n",
    "train_feature, train_label = next(iter(train_dataloader))\n",
    "visualize(train_feature, train_label, train_label)\n",
    "train_feature.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 256, 256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FPNNetwork(nn.Module):\n",
    "    def __init__(self, numberClass):\n",
    "        super().__init__()\n",
    "        _resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = create_feature_extractor(\n",
    "            _resnet50,\n",
    "            {\n",
    "                \"relu\": \"feat1\",\n",
    "                \"layer1\": \"feat2\",\n",
    "                \"layer2\": \"feat3\",\n",
    "                \"layer3\": \"feat4\",\n",
    "                \"layer4\": \"feat5\",\n",
    "            },\n",
    "        )\n",
    "        # # Freeze backbone\n",
    "        # for param in self.backbone.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        self.upsampling_2x_bilinear = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        self.upsampling_4x_bilinear = nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "        self.upsampling_8x_bilinear = nn.UpsamplingBilinear2d(scale_factor=8)\n",
    "        self.conv5_1x1 = nn.Conv2d(\n",
    "            in_channels=2048,\n",
    "            out_channels=256,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.conv5_3x3_1 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv5_3x3_2 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv4_1x1 = nn.Conv2d(\n",
    "            in_channels=1024,\n",
    "            out_channels=256,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.conv4_3x3_1 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv4_3x3_2 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv3_1x1 = nn.Conv2d(\n",
    "            in_channels=512,\n",
    "            out_channels=256,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.conv3_3x3_1 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv3_3x3_2 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv2_1x1 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=256,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.conv2_3x3_1 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv2_3x3_2 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.final_conv_1 = nn.Conv2d(\n",
    "            in_channels=512,\n",
    "            out_channels=512,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.final_conv_2 = nn.Conv2d(\n",
    "            in_channels=512,\n",
    "            out_channels=numberClass,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_output = self.backbone(x)\n",
    "        feat2, feat3, feat4, feat5 = (\n",
    "            backbone_output[\"feat2\"],\n",
    "            backbone_output[\"feat3\"],\n",
    "            backbone_output[\"feat4\"],\n",
    "            backbone_output[\"feat5\"],\n",
    "        )\n",
    "        conv5_mid = self.conv5_1x1(feat5).relu()\n",
    "        conv5_prediction = self.conv5_3x3_1(conv5_mid).relu()\n",
    "        conv5_prediction = self.conv5_3x3_2(conv5_prediction).relu()\n",
    "\n",
    "        conv4_lateral = self.conv4_1x1(feat4).relu()\n",
    "        conv4_mid = conv4_lateral + self.upsampling_2x_bilinear(conv5_mid)\n",
    "        conv4_prediction = self.conv4_3x3_1(conv4_mid).relu()\n",
    "        conv4_prediction = self.conv4_3x3_2(conv4_prediction).relu()\n",
    "\n",
    "        conv3_lateral = self.conv3_1x1(feat3).relu()\n",
    "        conv3_mid = conv3_lateral + self.upsampling_2x_bilinear(conv4_mid)\n",
    "        conv3_prediction = self.conv3_3x3_1(conv3_mid).relu()\n",
    "        conv3_prediction = self.conv3_3x3_2(conv3_prediction).relu()\n",
    "\n",
    "        conv2_lateral = self.conv2_1x1(feat2).relu()\n",
    "        conv2_mid = conv2_lateral + self.upsampling_2x_bilinear(conv3_mid)\n",
    "        conv2_prediction = self.conv2_3x3_1(conv2_mid).relu()\n",
    "        conv2_prediction = self.conv2_3x3_2(conv2_prediction).relu()\n",
    "\n",
    "        final_prediction_5 = self.upsampling_8x_bilinear(conv5_prediction)\n",
    "        final_prediction_4 = self.upsampling_4x_bilinear(conv4_prediction)\n",
    "        final_prediction_3 = self.upsampling_2x_bilinear(conv3_prediction)\n",
    "        final_prediction_2 = conv2_prediction\n",
    "\n",
    "        concatenated_prediction = torch.concatenate(\n",
    "            [\n",
    "                final_prediction_5,\n",
    "                final_prediction_4,\n",
    "                final_prediction_3,\n",
    "                final_prediction_2,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        concatenated_prediction = self.final_conv_1(concatenated_prediction).relu()\n",
    "        concatenated_prediction = self.final_conv_2(concatenated_prediction).relu()\n",
    "        return self.upsampling_4x_bilinear(concatenated_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UNETNetwork(nn.Module):\n",
    "    def __init__(self, numberClass):\n",
    "        super().__init__()\n",
    "        _resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = create_feature_extractor(\n",
    "            _resnet50,\n",
    "            {\n",
    "                \"layer1\": \"feat2\",\n",
    "                \"layer2\": \"feat3\",\n",
    "                \"layer3\": \"feat4\",\n",
    "                \"layer4\": \"feat5\",\n",
    "            },\n",
    "        )\n",
    "        self.upsampling_2x_bilinear = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=2048, out_channels=1024, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv6 = nn.Conv2d(\n",
    "            in_channels=1024, out_channels=512, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv7 = nn.Conv2d(\n",
    "            in_channels=512, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv8 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=128, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.convfinal = nn.Conv2d(\n",
    "            in_channels=128, out_channels=numberClass, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_output = self.backbone(x)\n",
    "        feat2, feat3, feat4, feat5 = (\n",
    "            backbone_output[\"feat2\"],\n",
    "            backbone_output[\"feat3\"],\n",
    "            backbone_output[\"feat4\"],\n",
    "            backbone_output[\"feat5\"],\n",
    "        )\n",
    "        feat4to6 = self.upsampling_2x_bilinear(self.conv5(feat5).relu())\n",
    "        feat3to7 = self.upsampling_2x_bilinear(self.conv6(feat4 + feat4to6).relu())\n",
    "        feat2to8 = self.upsampling_2x_bilinear(self.conv7(feat3 + feat3to7).relu())\n",
    "        featout = self.upsampling_2x_bilinear(self.conv8(feat2 + feat2to8).relu())\n",
    "        return self.upsampling_2x_bilinear(self.convfinal(featout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MovingMeanDiceLoss(torch.nn.Module):\n",
    "#     def __init__(self, n_class: int, alpha: float = 0.5):\n",
    "#         super().__init__()\n",
    "#         self.n_class = torch.nn.Parameter(\n",
    "#             torch.tensor(n_class),\n",
    "#             requires_grad=False,\n",
    "#         )\n",
    "#         self.alpha = torch.nn.Parameter(\n",
    "#             torch.tensor(alpha),\n",
    "#             requires_grad=False,\n",
    "#         )\n",
    "#         self.running_mean_weight = torch.nn.Parameter(\n",
    "#             torch.full(\n",
    "#                 [n_class],\n",
    "#                 fill_value=1 / n_class,\n",
    "#             ),\n",
    "#             requires_grad=False,\n",
    "#         )\n",
    "\n",
    "#     def calculate_weight(self, target: torch.Tensor):\n",
    "#         distribution_single = target.sum([0, 2, 3]) / target.sum()\n",
    "#         self.running_mean_weight.copy_(\n",
    "#             self.alpha * self.running_mean_weight\n",
    "#             + (1 - self.alpha) * distribution_single\n",
    "#         )\n",
    "\n",
    "#     def forward(\n",
    "#         self, pred: torch.Tensor, target: torch.Tensor, epsilon=1e-9\n",
    "#     ) -> torch.Tensor:\n",
    "#         pred_flat = pred.permute([1, 0, 2, 3]).flatten(1)\n",
    "#         label_flat = target.permute([1, 0, 2, 3]).flatten(1)\n",
    "#         intersection = 2 * torch.sum(pred_flat * label_flat, dim=1)\n",
    "#         total_area = torch.sum(pred_flat, dim=1) + torch.sum(label_flat, dim=1)\n",
    "#         dice_index = (intersection + epsilon) / (total_area + epsilon)\n",
    "#         dice_loss = 1 - dice_index\n",
    "#         weighted_dice_loss = dice_loss * torch.softmax(\n",
    "#             1 - self.running_mean_weight + epsilon, dim=0\n",
    "#         )\n",
    "\n",
    "#         self.calculate_weight(target=target)\n",
    "#         return weighted_dice_loss.mean()\n",
    "\n",
    "\n",
    "# training_data = UAVIDDataset(\n",
    "#     path=\"data/processed_dataset/\",\n",
    "#     is_train=True,\n",
    "# )\n",
    "# train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True)\n",
    "# train_feature, train_label = next(iter(train_dataloader))\n",
    "# sample_pred = torch.rand_like(train_label).softmax(1)\n",
    "# visualize(train_feature, train_label, sample_pred)\n",
    "# train_feature.size()\n",
    "\n",
    "# model = UNETNetwork(numberClass=8)\n",
    "\n",
    "# movingLoss = MovingMeanDiceLoss(n_class=8)\n",
    "# for x, y in train_dataloader:\n",
    "#     output = model(x)\n",
    "#     loss = movingLoss(output.softmax(1), y)\n",
    "#     print(loss)\n",
    "#     loss.backward()\n",
    "\n",
    "#     # break\n",
    "#     # movingLoss(sample_pred.softmax(1), y)\n",
    "#     # print(torch.round(movingLoss.running_mean_weight, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FPNNetwork(nn.Module):\n",
    "    def __init__(self, numberClass):\n",
    "        super().__init__()\n",
    "        _resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = create_feature_extractor(\n",
    "            _resnet50,\n",
    "            {\n",
    "                \"layer1\": \"feat2\",\n",
    "                \"layer2\": \"feat3\",\n",
    "                \"layer3\": \"feat4\",\n",
    "                \"layer4\": \"feat5\",\n",
    "            },\n",
    "        )\n",
    "        self.upsampling_2x_bilinear = nn.Upsample(scale_factor=2)\n",
    "        self.upsampling_4x_bilinear = nn.Upsample(scale_factor=4)\n",
    "        self.upsampling_8x_bilinear = nn.Upsample(scale_factor=8)\n",
    "        self.upsampling_16x_bilinear = nn.Upsample(scale_factor=16)\n",
    "        self.upsampling_2x_bilinear = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=2048, out_channels=1024, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv6 = nn.Conv2d(\n",
    "            in_channels=1024, out_channels=512, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv7 = nn.Conv2d(\n",
    "            in_channels=512, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv8 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=128, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.convfinal = nn.Conv2d(\n",
    "            in_channels=8, out_channels=numberClass, kernel_size=1\n",
    "        )\n",
    "\n",
    "        self.output_conv1 = nn.Conv2d(\n",
    "            in_channels=1024, out_channels=numberClass, kernel_size=1, padding=\"valid\"\n",
    "        )\n",
    "        self.output_conv2 = nn.Conv2d(\n",
    "            in_channels=512, out_channels=numberClass, kernel_size=1, padding=\"valid\"\n",
    "        )\n",
    "        self.output_conv3 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=numberClass, kernel_size=1, padding=\"valid\"\n",
    "        )\n",
    "        self.output_conv4 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=numberClass, kernel_size=1, padding=\"valid\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_output = self.backbone(x)\n",
    "        feat2, feat3, feat4, feat5 = (\n",
    "            backbone_output[\"feat2\"],\n",
    "            backbone_output[\"feat3\"],\n",
    "            backbone_output[\"feat4\"],\n",
    "            backbone_output[\"feat5\"],\n",
    "        )\n",
    "\n",
    "        #         print(feat2.size())\n",
    "        #         print(feat3.size())\n",
    "        #         print(feat4.size())\n",
    "        #         print(feat5.size())\n",
    "\n",
    "        middle1 = self.upsampling_2x_bilinear(self.conv5(feat5).relu())\n",
    "        middle2 = self.upsampling_2x_bilinear(self.conv6(feat4 + middle1).relu())\n",
    "        middle3 = self.upsampling_2x_bilinear(self.conv7(feat3 + middle2).relu())\n",
    "        middle4 = self.upsampling_2x_bilinear(self.conv8(feat2 + middle3).relu())\n",
    "\n",
    "        #         print('middle')\n",
    "        #         print(middle1.size())\n",
    "        #         print(middle2.size())\n",
    "        #         print(middle3.size())\n",
    "        #         print(middle4.size())\n",
    "\n",
    "        output1 = self.upsampling_8x_bilinear(self.output_conv1(middle1).relu())\n",
    "        output2 = self.upsampling_4x_bilinear(self.output_conv2(middle2).relu())\n",
    "        output3 = self.upsampling_2x_bilinear(self.output_conv3(middle3).relu())\n",
    "        output4 = self.output_conv4(middle4)\n",
    "\n",
    "        #         print('output')\n",
    "        #         print(output1.size())\n",
    "        #         print(output2.size())\n",
    "        #         print(output3.size())\n",
    "        #         print(output4.size())\n",
    "\n",
    "        return self.upsampling_2x_bilinear(\n",
    "            self.convfinal(output1 + output2 + output3 + output4)\n",
    "        )\n",
    "\n",
    "        # return finaloutput = self.upsampling_2x_bilinear(self.convfinal(featout))\n",
    "        # return self.upsampling_4x_bilinear(self.convfinal(self.upsampling_8x_bilinear(self.conv5(feat5)) + feat2))\n",
    "\n",
    "\n",
    "# model = FPNNetwork(8)\n",
    "# with torch.no_grad():\n",
    "#     print(model(torch.rand(1, 3, 256, 256)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SmallerFPNNetwork(nn.Module):\n",
    "#     def __init__(self, numberClass):\n",
    "#         super().__init__()\n",
    "#         mobilenet = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
    "#         self.backbone = create_feature_extractor(\n",
    "#             mobilenet,\n",
    "#             {\n",
    "#                 \"features.1\": \"feat2\",\n",
    "#                 \"features.3\": \"feat3\",\n",
    "#                 \"features.8\": \"feat4\",\n",
    "#                 \"features.12\": \"feat5\",\n",
    "#             },\n",
    "#         )\n",
    "#         self.upsampling_2x_bilinear = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "#         self.upsampling_4x_bilinear = nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "#         self.upsampling_8x_bilinear = nn.UpsamplingBilinear2d(scale_factor=8)\n",
    "#         self.upsampling_16x_bilinear = nn.UpsamplingBilinear2d(scale_factor=16)\n",
    "#         # self.conv5 = nn.Conv2d(\n",
    "#         #     in_channels=2048, out_channels=64, kernel_size=3, padding=1\n",
    "#         # )\n",
    "#         self.conv5 = nn.Conv2d(\n",
    "#             in_channels=576, out_channels=16, kernel_size=3, padding=1\n",
    "#         )\n",
    "#         self.conv6 = nn.Conv2d(\n",
    "#             in_channels=1024, out_channels=512, kernel_size=3, padding=1\n",
    "#         )\n",
    "#         self.conv7 = nn.Conv2d(\n",
    "#             in_channels=512, out_channels=256, kernel_size=3, padding=1\n",
    "#         )\n",
    "#         self.conv8 = nn.Conv2d(\n",
    "#             in_channels=24, out_channels=16, kernel_size=3, padding=1\n",
    "#         )\n",
    "#         # self.convfinal = nn.Conv2d(\n",
    "#         #     in_channels=64, out_channels=numberClass, kernel_size=1\n",
    "#         # )\n",
    "#         self.convfinal = nn.Conv2d(\n",
    "#             in_channels=16, out_channels=numberClass, kernel_size=1\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         backbone_output = self.backbone(x)\n",
    "#         feat2, feat3, feat4, feat5 = (\n",
    "#             # backbone_output[\"feat1\"],\n",
    "#             backbone_output[\"feat2\"],\n",
    "#             backbone_output[\"feat3\"],\n",
    "#             backbone_output[\"feat4\"],\n",
    "#             backbone_output[\"feat5\"],\n",
    "#         )\n",
    "\n",
    "#         # print(feat1.size())\n",
    "#         # print(feat2.size())\n",
    "#         # print(feat3.size())\n",
    "#         # print(feat4.size())\n",
    "#         # print(feat5.size())\n",
    "\n",
    "#         # return self.upsampling_2x_bilinear(\n",
    "#         #     self.convfinal(self.upsampling_16x_bilinear(self.conv5(feat5)) + feat1)\n",
    "#         # )\n",
    "\n",
    "#         return self.upsampling_4x_bilinear(\n",
    "#             self.convfinal(\n",
    "#                 self.upsampling_8x_bilinear(self.conv5(feat5).relu())\n",
    "#                 + feat2\n",
    "#                 + self.upsampling_2x_bilinear(self.conv8(feat3).relu())\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         # feat4to6 = self.upsampling_2x_bilinear(self.conv5(feat5).relu())\n",
    "#         # feat3to7 = self.upsampling_2x_bilinear(self.conv6(feat4 + feat4to6).relu())\n",
    "#         # feat2to8 = self.upsampling_2x_bilinear(self.conv7(feat3 + feat3to7).relu())\n",
    "#         # featout = self.upsampling_2x_bilinear(self.conv8(feat2 + feat2to8).relu())\n",
    "#         # return self.upsampling_2x_bilinear(self.convfinal(featout))\n",
    "\n",
    "\n",
    "# # model = SmallerFPNNetwork(8)\n",
    "# # with torch.no_grad():\n",
    "# #     print(model(torch.rand(1, 3, 256, 256)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialFPNNetwork(nn.Module):\n",
    "    def __init__(self, numberClass):\n",
    "        super().__init__()\n",
    "        _resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = create_feature_extractor(\n",
    "            _resnet50,\n",
    "            {\n",
    "                \"relu\": \"feat1\",\n",
    "                \"layer1\": \"feat2\",\n",
    "                \"layer2\": \"feat3\",\n",
    "                \"layer3\": \"feat4\",\n",
    "                \"layer4\": \"feat5\",\n",
    "            },\n",
    "        )\n",
    "        self.upsampling_2x_bilinear = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        self.upsampling_4x_bilinear = nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "        self.upsampling_8x_bilinear = nn.UpsamplingBilinear2d(scale_factor=8)\n",
    "        self.upsampling_16x_bilinear = nn.UpsamplingBilinear2d(scale_factor=16)\n",
    "        # self.conv5 = nn.Conv2d(\n",
    "        #     in_channels=2048, out_channels=64, kernel_size=3, padding=1\n",
    "        # )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=2048, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv6 = nn.Conv2d(\n",
    "            in_channels=1024, out_channels=512, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv7 = nn.Conv2d(\n",
    "            in_channels=1024, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv8 = nn.Conv2d(\n",
    "            in_channels=512, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.convfinal = nn.Conv2d(\n",
    "            in_channels=256, out_channels=numberClass, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_output = self.backbone(x)\n",
    "        feat1, feat2, feat3, feat4, feat5 = (\n",
    "            backbone_output[\"feat1\"],\n",
    "            backbone_output[\"feat2\"],\n",
    "            backbone_output[\"feat3\"],\n",
    "            backbone_output[\"feat4\"],\n",
    "            backbone_output[\"feat5\"],\n",
    "        )\n",
    "\n",
    "        # print(feat1.size())\n",
    "        # print(feat2.size())\n",
    "        # print(feat3.size())\n",
    "        # print(feat4.size())\n",
    "        # print(feat5.size())\n",
    "\n",
    "        return self.upsampling_4x_bilinear(\n",
    "            self.convfinal(\n",
    "                self.upsampling_8x_bilinear(self.conv5(feat5).relu())\n",
    "                + self.upsampling_4x_bilinear(self.conv7(feat4).relu())\n",
    "                + self.upsampling_2x_bilinear(self.conv8(feat3).relu())\n",
    "                + feat2\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "# model = SpecialFPNNetwork(8)\n",
    "# with torch.no_grad():\n",
    "#     print(model(torch.rand(1, 3, 256, 256)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_index(\n",
    "    pred: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    epsilon=1e-9,\n",
    "):\n",
    "    pred_flat = pred.flatten()\n",
    "    target_flat = target.flatten()\n",
    "    nominator = 2 * torch.matmul(pred_flat, target_flat)\n",
    "    denominator = torch.sum(pred_flat) + torch.sum(target_flat)\n",
    "    return (nominator + epsilon) / (denominator + epsilon)\n",
    "\n",
    "\n",
    "def dice_index_per_channel(\n",
    "    pred: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    epsilon=1e-9,\n",
    "):\n",
    "    pred_flat = pred.permute([1, 0, 2, 3]).flatten(1)\n",
    "    label_flat = target.permute([1, 0, 2, 3]).flatten(1)\n",
    "    nominator = 2 * torch.sum(pred_flat * label_flat, dim=1)\n",
    "    denominator = torch.sum(pred_flat, dim=1) + torch.sum(label_flat, dim=1)\n",
    "    return (nominator + epsilon) / (denominator + epsilon)\n",
    "\n",
    "\n",
    "def total_loss(pred: torch.Tensor, target: torch.Tensor):\n",
    "    crossentropy_loss = torch.nn.functional.cross_entropy(pred, target)\n",
    "    dice_loss = 1 - dice_index(pred.softmax(1), target)\n",
    "    return crossentropy_loss + dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = UAVIDDataset(\n",
    "    path=\"data/processed_dataset/\",\n",
    "    is_train=True,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "nclass = 8\n",
    "device = torch.device('mps')\n",
    "test_data = UAVIDDataset(path=\"data/processed_dataset/\", is_train=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=4)\n",
    "model = SpecialFPNNetwork(numberClass=nclass).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]).to(device)\n",
    "timestamp = datetime.datetime.now().strftime(r\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(\"data/training/train_FPN_{}\".format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('data/savedmodel/49_model.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    model: torch.nn.Module,\n",
    "    test_dataloader: DataLoader,\n",
    "    n_class: int,\n",
    ") -> Tuple[float, NDArray]:\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "    avg_dice = np.zeros(n_class)\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_dataloader):\n",
    "            inputs: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            inputs = normalize(inputs)\n",
    "            outputs = model(inputs)\n",
    "            avg_loss += total_loss(outputs, labels).item()\n",
    "            avg_dice += dice_index_per_channel(outputs.softmax(1), labels).tolist()\n",
    "    return avg_loss / len(test_dataloader), avg_dice / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(0, 200):\n",
    "    # Training\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_dice = np.zeros(nclass)\n",
    "    for idx, data in enumerate(tqdm(train_dataloader)):\n",
    "        inputs: torch.Tensor\n",
    "        labels: torch.Tensor\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        inputs = normalize(inputs)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = total_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        dice_per_channel = dice_index_per_channel(outputs.softmax(1), labels)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_dice += dice_per_channel.tolist()\n",
    "\n",
    "        if idx % 100 == 99:\n",
    "            current_training_sample = epoch * len(train_dataloader) + idx + 1\n",
    "            current_dice = running_dice / 100\n",
    "            writer.add_scalar(\"Loss/train\", running_loss / 100, current_training_sample)\n",
    "            writer.add_scalars(\n",
    "                \"dice/train\",\n",
    "                {name: current_dice[i] for i, name in enumerate(UAVIDDataset.dataset_labels)},\n",
    "                current_training_sample,\n",
    "            )\n",
    "            print(f\"Loss: {running_loss / 100}\")\n",
    "            running_loss = 0.0\n",
    "            running_dice *= 0\n",
    "\n",
    "        break\n",
    "\n",
    "    # Testing\n",
    "    test_loss, test_dice = test_model(model, test_dataloader, nclass)\n",
    "    writer.add_scalar(\"train/\", test_loss)\n",
    "    # writer.add_scalars(\n",
    "    #         \"dice/train\",\n",
    "    #         {name: current_dice[i] for i, name in enumerate(UAVIDDataset.dataset_labels)},\n",
    "    #         current_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"fpn_200_epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = UAVIDDataset(path=\"/mnt/processed_dataset/\", is_train=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "test_iterator = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_feature, test_label = next(test_iterator)\n",
    "with torch.no_grad():\n",
    "    outputs = model(normalize(test_feature.to(\"cuda\")))\n",
    "    outputs = outputs.to(\"cpu\").softmax(axis=1)\n",
    "    visualize(test_feature, test_label, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_positional_encoding(embedding_length, word_length):\n",
    "    half_embedding_length = embedding_length // 2\n",
    "\n",
    "    across_word = (\n",
    "        torch.arange(0, word_length).unsqueeze(0).T.repeat([1, half_embedding_length])\n",
    "    )\n",
    "    across_embedding = (\n",
    "        torch.arange(half_embedding_length).unsqueeze(0).repeat([word_length, 1])\n",
    "    )\n",
    "\n",
    "    sin_positional = torch.sin(\n",
    "        across_word / torch.pow(10000, across_embedding * 2 / half_embedding_length)\n",
    "    )\n",
    "    cos_positional = torch.cos(\n",
    "        across_word / torch.pow(10000, across_embedding * 2 / half_embedding_length)\n",
    "    )\n",
    "    output = torch.concat([sin_positional, cos_positional]).T.flatten()\n",
    "    return torch.stack(torch.split(output, word_length), dim=1).reshape(word_length, -1)\n",
    "\n",
    "\n",
    "class PositionEncoding(torch.nn.Module):\n",
    "    def __init__(self, embedding_length: int, max_word_length: int):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = nn.Parameter(\n",
    "            generate_positional_encoding(\n",
    "                embedding_length,\n",
    "                max_word_length,\n",
    "            ).unsqueeze(1),\n",
    "            requires_grad=False,\n",
    "        )\n",
    "        self.embedding_length = embedding_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[0 : x.shape[0]]\n",
    "\n",
    "\n",
    "class AttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, embedding_length: int, word_length: int):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = PositionEncoding(16 * 16 * 3, 3)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            16 * 16 * 3,\n",
    "            8,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        attn_output, _ = self.attention(x, x, x, need_weights=False)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class AttentionNetwork(nn.Module):\n",
    "    def __init__(self, numberClass):\n",
    "        super().__init__()\n",
    "        _resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = create_feature_extractor(\n",
    "            _resnet50,\n",
    "            {\n",
    "                \"relu\": \"feat1\",\n",
    "                \"layer1\": \"feat2\",\n",
    "                \"layer2\": \"feat3\",\n",
    "                \"layer3\": \"feat4\",\n",
    "                \"layer4\": \"feat5\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        self.layer_1_conv = torch.nn.Conv2d(64, out_channels=3, kernel_size=1)\n",
    "        self.layer_2_conv = torch.nn.Conv2d(512, out_channels=3, kernel_size=1)\n",
    "        self.layer_3_conv = torch.nn.Conv2d(2048, out_channels=3, kernel_size=1)\n",
    "        self.attention_block = AttentionBlock(16 * 16 * 3, 3)\n",
    "        self.conv_1 = nn.Conv2d(\n",
    "            in_channels=2048, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv_2 = nn.Conv2d(\n",
    "            in_channels=512, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv_3 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.middle_embedding = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=256,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.convfinal = nn.Conv2d(\n",
    "            in_channels=256, out_channels=numberClass, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_output = self.backbone(x)\n",
    "        feat1, feat2, feat3, feat4, feat5 = (\n",
    "            backbone_output[\"feat1\"],\n",
    "            backbone_output[\"feat2\"],\n",
    "            backbone_output[\"feat3\"],\n",
    "            backbone_output[\"feat4\"],\n",
    "            backbone_output[\"feat5\"],\n",
    "        )\n",
    "\n",
    "        # Only address attention for 3 layers\n",
    "        # feat1, feat3, feat5\n",
    "        layer_1 = torch.nn.functional.avg_pool2d(\n",
    "            self.layer_1_conv(feat1),\n",
    "            kernel_size=8,\n",
    "        )\n",
    "        layer_2 = torch.nn.functional.avg_pool2d(\n",
    "            self.layer_2_conv(feat3),\n",
    "            kernel_size=2,\n",
    "        )\n",
    "        layer_3 = torch.nn.functional.interpolate(\n",
    "            self.layer_3_conv(feat5),\n",
    "            size=16,\n",
    "        )\n",
    "\n",
    "        batch_size = layer_1.shape[0]\n",
    "        layer_1 = layer_1.reshape(batch_size, -1)\n",
    "        layer_2 = layer_2.reshape(batch_size, -1)\n",
    "        layer_3 = layer_3.reshape(batch_size, -1)\n",
    "        combined_output = torch.stack([layer_1, layer_2, layer_3])\n",
    "        attention_output = self.attention_block(combined_output)\n",
    "\n",
    "        output_1 = self.middle_embedding(\n",
    "            torch.nn.functional.interpolate(\n",
    "                attention_output[0].reshape((batch_size, 3, 16, 16)),\n",
    "                scale_factor=4,\n",
    "            )\n",
    "        )\n",
    "        output_2 = self.middle_embedding(\n",
    "            torch.nn.functional.interpolate(\n",
    "                attention_output[1].reshape((batch_size, 3, 16, 16)),\n",
    "                scale_factor=4,\n",
    "            )\n",
    "        )\n",
    "        output_3 = self.middle_embedding(\n",
    "            torch.nn.functional.interpolate(\n",
    "                attention_output[2].reshape((batch_size, 3, 16, 16)),\n",
    "                scale_factor=4,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return self.convfinal(\n",
    "            nn.functional.upsample_bilinear(\n",
    "                (\n",
    "                    self.conv_1(\n",
    "                        nn.functional.interpolate(\n",
    "                            feat5, scale_factor=8, mode=\"bilinear\"\n",
    "                        )\n",
    "                    )\n",
    "                    + output_3\n",
    "                ).relu()\n",
    "                + (\n",
    "                    self.conv_2(\n",
    "                        nn.functional.interpolate(\n",
    "                            feat3, scale_factor=2, mode=\"bilinear\"\n",
    "                        )\n",
    "                    )\n",
    "                    + output_2\n",
    "                ).relu()\n",
    "                + (self.conv_3(feat2) + output_1).relu(),\n",
    "                scale_factor=4,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "model = AttentionNetwork(8)\n",
    "with torch.no_grad():\n",
    "    output = model(torch.rand(1, 3, 256, 256))\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    plt.imshow(output[0, 0], interpolation=\"nearest\", aspect=\"auto\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

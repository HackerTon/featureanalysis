{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64c643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448e19e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import functools as ft\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import segmentation_models as sm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from preprocessing import UavidDataset\n",
    "\n",
    "sm.set_framework(\"tf.keras\")\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "tf.random.set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77180702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_decode(image, label):\n",
    "    image = tf.io.read_file(image, \"image\")\n",
    "    label = tf.io.read_file(label, \"label\")\n",
    "\n",
    "    image = tf.image.decode_image(image)\n",
    "    label = tf.image.decode_image(label)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# [w, h, c], 448, 448, 3\n",
    "def decode_crop(image, label):\n",
    "    image = image[368 // 2 : -(368 // 2), 256 // 2 : -(256 // 2)]\n",
    "    label = label[368 // 2 : -(368 // 2), 256 // 2 : -(256 // 2)]\n",
    "\n",
    "    img_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for index in range(4 * 8):\n",
    "        x, y = index // 8, index % 8\n",
    "        img_array.append(image[448 * x : 448 * (1 + x), 448 * y : 448 * (1 + y)])\n",
    "        label_array.append(label[448 * x : 448 * (1 + x), 448 * y : 448 * (1 + y)])\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((img_array, label_array))\n",
    "\n",
    "\n",
    "def get_mask(image, label):\n",
    "    labels = []\n",
    "    labels.append((label[:, :, 0] == 0) & (label[:, :, 1] == 0) & (label[:, :, 2] == 0))\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 0) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 64) & (label[:, :, 2] == 128)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 0) & (label[:, :, 1] == 128) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 128) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 64) & (label[:, :, 1] == 0) & (label[:, :, 2] == 128)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 192) & (label[:, :, 1] == 0) & (label[:, :, 2] == 192)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 64) & (label[:, :, 1] == 64) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    # must perform this\n",
    "    return image, tf.transpose(labels, [1, 2, 0])\n",
    "\n",
    "\n",
    "def create_ds(batch_size, istrain=True, maximage=False):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    if istrain:\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release_image/uavid_train/**/Images/*.png\"\n",
    "        images = glob.glob(directory, recursive=True)\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release_image/uavid_train/**/Labels/*.png\"\n",
    "        labels = glob.glob(directory, recursive=True)\n",
    "    else:\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release_image/uavid_val/**/Images/*.png\"\n",
    "        images = glob.glob(directory, recursive=True)\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release_image/uavid_val/**/Labels/*.png\"\n",
    "        labels = glob.glob(directory, recursive=True)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    ds = ds.cache()\n",
    "    if istrain:\n",
    "        ds = ds.shuffle(6400, SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(get_image_decode, AUTOTUNE)\n",
    "\n",
    "    if not maximage:\n",
    "        ds = ds.flat_map(decode_crop)\n",
    "\n",
    "    ds = ds.map(get_mask, AUTOTUNE)\n",
    "\n",
    "    # batch and prefetch\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    if istrain:\n",
    "        ds = ds.prefetch(AUTOTUNE)\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0bc5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_backbone_efficient():\n",
    "    _backbone = keras.applications.EfficientNetB0(include_top=False)\n",
    "\n",
    "    outputs = [\n",
    "        layer.output\n",
    "        for layer in _backbone.layers\n",
    "        if layer.name\n",
    "        in [\n",
    "            \"block2a_activation\",\n",
    "            \"block3a_activation\",\n",
    "            \"block5a_activation\",\n",
    "            \"block7a_activation\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[_backbone.input], outputs=outputs, name=\"efficientb0_backbone\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ef5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(tf.keras.layers.Layer):\n",
    "    def __init__(self, backbone=None, **kwargs):\n",
    "        super().__init__(name=\"Feature_Pyramid_Network\", **kwargs)\n",
    "\n",
    "        self.backbone = create_backbone_efficient()\n",
    "\n",
    "        self.conv5_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv4_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv3_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv2_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv5_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv5_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv4_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv4_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        # 112x112, 56x56, 28x28, 14x14\n",
    "        conv2, conv3, conv4, conv5 = self.backbone(images, training=False)\n",
    "        conv5_m = self.conv5_1x1(conv5)\n",
    "        conv5_p = self.conv5_3x3_1(conv5_m)\n",
    "        conv5_p = self.conv5_3x3_2(conv5_p)\n",
    "\n",
    "        conv4_m_1 = self.upscale(conv5_m)\n",
    "        conv4_m_2 = self.conv4_1x1(conv4)\n",
    "        conv4_m = conv4_m_1 + conv4_m_2\n",
    "        conv4_p = self.conv4_3x3_1(conv4_m)\n",
    "        conv4_p = self.conv4_3x3_2(conv4_p)\n",
    "\n",
    "        conv3_m_1 = self.upscale(conv4_m)\n",
    "        conv3_m_2 = self.conv3_1x1(conv3)\n",
    "        conv3_m = conv3_m_1 + conv3_m_2\n",
    "        conv3_p = self.conv3_3x3_1(conv3_m)\n",
    "        conv3_p = self.conv3_3x3_2(conv3_p)\n",
    "\n",
    "        conv2_m_1 = self.upscale(conv3_m)\n",
    "        conv2_m_2 = self.conv2_1x1(conv2)\n",
    "        conv2_m = conv2_m_1 + conv2_m_2\n",
    "        conv2_p = self.conv2_3x3_1(conv2_m)\n",
    "        conv2_p = self.conv2_3x3_2(conv2_p)\n",
    "\n",
    "        return conv5_p, conv4_p, conv3_p, conv2_p\n",
    "\n",
    "\n",
    "class FCN(tf.keras.Model):\n",
    "    def __init__(self, n_classes=8, backbone=None, **kwargs):\n",
    "        super().__init__(name=\"FCN\", **kwargs)\n",
    "        self.fpn = FPN(backbone)\n",
    "        self.upscale_2x = tf.keras.layers.UpSampling2D()\n",
    "        self.upscale_4x = tf.keras.layers.UpSampling2D((4, 4))\n",
    "        self.upscale_8x = tf.keras.layers.UpSampling2D((8, 8))\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.conv6 = tf.keras.layers.Conv2D(\n",
    "            filters=(512), kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv7 = tf.keras.layers.Conv2D(\n",
    "            filters=n_classes, kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale_final = tf.keras.layers.UpSampling2D(\n",
    "            size=(4, 4), interpolation=\"bilinear\"\n",
    "        )\n",
    "        self.final_activation = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        conv5_p, conv4_p, conv3_p, conv2_p = self.fpn(images, training=training)\n",
    "        m_5 = self.upscale_8x(conv5_p)\n",
    "        m_4 = self.upscale_4x(conv4_p)\n",
    "        m_3 = self.upscale_2x(conv3_p)\n",
    "        m_2 = conv2_p\n",
    "\n",
    "        m_all = self.concat([m_2, m_3, m_4, m_5])\n",
    "        m_all = self.conv6(m_all)\n",
    "        m_all = self.conv7(m_all)\n",
    "        m_all = self.upscale_final(m_all)\n",
    "        m_all = self.final_activation(m_all)\n",
    "\n",
    "        return m_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebebd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_ORIG(tf.keras.Model):\n",
    "    def __init__(self, n_classes=8, backbone=None, **kwargs):\n",
    "        super().__init__(name=\"FCN_ORIG\", **kwargs)\n",
    "\n",
    "        self.backbone = create_backbone_efficient()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale2x_1 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale2x_2 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale2x_3 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale2x_4 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(4, 4),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.final_activation = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        conv1_o, conv2_o, conv3_o, conv4_o = self.backbone(images, training=False)\n",
    "        conv1_o = self.conv1(conv1_o)\n",
    "        conv2_o = self.conv2(conv2_o)\n",
    "        conv3_o = self.conv3(conv3_o)\n",
    "\n",
    "        fcn_16x = self.upscale2x_1(conv4_o) + conv3_o\n",
    "        fcn_8x = self.upscale2x_2(fcn_16x) + conv2_o\n",
    "        fcn_4x = self.upscale2x_3(fcn_8x) + conv1_o\n",
    "        final_output = self.upscale2x_4(fcn_4x)\n",
    "        final_output = self.final_activation(final_output)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6385667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network must OUTPUT in logits [-inf, inf]\n",
    "# make sure input dimension is [B, H, W, C]\n",
    "def jindex_per_class(target, pred):\n",
    "    intersection = tf.reduce_sum(target * pred, [0, 1, 2])\n",
    "    union = tf.reduce_sum(target + pred, [0, 1, 2]) - intersection\n",
    "    return (intersection + 1e-9) / (union + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e43513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 21:58:50.211576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-29 21:58:50.226512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-29 21:58:50.227414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-29 21:58:50.229676: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-29 21:58:50.230781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-29 21:58:50.231529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-29 21:58:50.232295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-29 21:58:51.123855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-29 21:58:51.124278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-29 21:58:51.124647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-29 21:58:51.124899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1621 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 950M, pci bus id: 0000:01:00.0, compute capability: 5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'webserver/trained_model/fpn/ckpt-20'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection = [\"fcn8s\", \"unet\", \"fpn\"]\n",
    "model_choice = 2\n",
    "name_model = model_selection[model_choice]\n",
    "\n",
    "n_classes = 8\n",
    "batch_size = 1\n",
    "trainds, testds = UavidDataset.create_ds(batch_size=batch_size)\n",
    "if model_choice == 0:\n",
    "    model = FCN_ORIG(n_classes)\n",
    "elif model_choice == 1:\n",
    "    model = sm.Unet(\n",
    "        backbone_name=\"efficientnetb0\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        encoder_freeze=False,\n",
    "        activation=\"softmax\",\n",
    "        classes=n_classes,\n",
    "        decoder_use_batchnorm=False,\n",
    "    )\n",
    "elif model_choice == 2:\n",
    "    model = FCN(8)\n",
    "else:\n",
    "    assert \"No model chosen\"\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckptmg = tf.train.CheckpointManager(ckpt, f\"webserver/trained_model/{name_model}\", 5)\n",
    "ckptmg.restore_or_initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d66a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 21:58:58.915508: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100\n",
      "2022-12-29 21:59:00.770478: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.77GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 445.26355s\n",
      "IoU: 0.26863\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 1.0\n",
    "\n",
    "# testing on train dataset and testing dataset\n",
    "train_amount = 6400 / batch_size\n",
    "test_amount = 2240 / batch_size\n",
    "\n",
    "iou_train = np.zeros([int(train_amount), 8])\n",
    "iou_test = np.zeros([int(test_amount), 8])\n",
    "\n",
    "# print(\"Training set...\")\n",
    "# time_init = time.time()\n",
    "# for idx, (bs_images, bs_labels) in trainds.enumerate():\n",
    "#     if model_choice == 1:\n",
    "#         bs_images = sm.get_preprocessing(\"efficientnetb0\")(bs_images)\n",
    "\n",
    "#     if model_choice == 1:\n",
    "#         output = model(bs_images, training=True)\n",
    "#     else:\n",
    "#         output = model(bs_images, training=False)\n",
    "#     iou_train[idx] = jindex_per_class(bs_labels, output)\n",
    "\n",
    "# time_taken = time.time() - time_init\n",
    "# print(f\"Time taken: {round(time_taken, 5)}s\")\n",
    "# print(f\"IoU: {round(np.mean(iou_train), 5)}\")\n",
    "\n",
    "# testing set\n",
    "print(\"Testing set...\")\n",
    "time_init = time.time()\n",
    "for idx, (bs_images, bs_labels) in testds.enumerate():\n",
    "    if model_choice == 1:\n",
    "        bs_images = sm.get_preprocessing(\"efficientnetb0\")(bs_images)\n",
    "\n",
    "    if model_choice == 1:\n",
    "        output = model(bs_images, training=True)\n",
    "    else:\n",
    "        output = model(bs_images, training=False)\n",
    "    iou_test[idx] = jindex_per_class(bs_labels, output)\n",
    "\n",
    "time_taken = time.time() - time_init\n",
    "print(f\"Time taken: {round(time_taken, 5)}s\")\n",
    "print(f\"IoU: {round(np.mean(iou_test), 5)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa559ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 1 artists>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x=['train'], height=[np.average(iou_train)], color='blue', align='center')\n",
    "plt.bar(x=['test'], height=[np.average(iou_test)], color='red', align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d5c9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1805c8efa0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(\n",
    "    x=UavidDataset.labels(),\n",
    "    height=np.average(iou_train, 0),\n",
    "    color=\"blue\",\n",
    "    width=-0.4,\n",
    "    align=\"edge\",\n",
    "    label=\"train\",\n",
    ")\n",
    "\n",
    "plt.bar(\n",
    "    x=UavidDataset.labels(),\n",
    "    height=np.average(iou_test, 0),\n",
    "    color=\"red\",\n",
    "    width=0.4,\n",
    "    align=\"edge\",\n",
    "    label=\"test\",\n",
    ")\n",
    "\n",
    "plt.ylim([0, 1.0])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728120dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "  return np.concatenate([[x[0]], x[1], x[2]])\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    list(map(convert, [[name_model, np.average(iou_train, 0), np.average(iou_test, 0)]])),\n",
    "    columns=[\n",
    "        \"model\",\n",
    "        \"trainc1\",\n",
    "        \"trainc2\",\n",
    "        \"trainc3\",\n",
    "        \"trainc4\",\n",
    "        \"trainc5\",\n",
    "        \"trainc6\",\n",
    "        \"trainc7\",\n",
    "        \"trainc8\",\n",
    "        \"testc1\",\n",
    "        \"testc2\",\n",
    "        \"testc3\",\n",
    "        \"testc4\",\n",
    "        \"testc5\",\n",
    "        \"testc6\",\n",
    "        \"testc7\",\n",
    "        \"testc8\",\n",
    "    ],\n",
    ")\n",
    "second = results.pop('model')\n",
    "\n",
    "results.iloc[:, 0:] = results.iloc[:, 0:].astype('float')\n",
    "results['model'] = second\n",
    "results.to_csv(f'results_{name_model})_uavid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0beed873570eadf18b27de988f74387134654fe26ad0c1ed6b53170102862c4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tf21')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

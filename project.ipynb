{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import segmentation_models as sm\n",
    "import functools as ft\n",
    "import pandas as pd\n",
    "\n",
    "# disable GPU computation\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "sm.set_framework('tf.keras')\n",
    "sm.framework()\n",
    "\n",
    "tf.random.set_seed(1024)\n",
    "SEED = 100"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def get_image_decode(image, label):\n",
    "    image = tf.io.read_file(image, \"image\")\n",
    "    label = tf.io.read_file(label, \"label\")\n",
    "\n",
    "    image = tf.image.decode_image(image)\n",
    "    label = tf.image.decode_image(label)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# [w, h, c], 448, 448, 3\n",
    "def decode_crop(image, label):\n",
    "    image = image[256 // 2 : -(256 // 2), 368 // 2 : -(368 // 2)]\n",
    "    label = label[256 // 2 : -(256 // 2), 368 // 2 : -(368 // 2)]\n",
    "\n",
    "    img_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for index in range(8 * 4):\n",
    "        x, y = index // 8, index % 4\n",
    "        img_array.append(image[448 * x : 448 * (1 + x), 448 * y : 448 * (1 + y)])\n",
    "        label_array.append(label[448 * x : 448 * (1 + x), 448 * y : 448 * (1 + y)])\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((img_array, label_array))\n",
    "\n",
    "\n",
    "def get_mask(image, label):\n",
    "    labels = []\n",
    "    labels.append((label[:, :, 0] == 0) & (label[:, :, 1] == 0) & (label[:, :, 2] == 0))\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 0) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 64) & (label[:, :, 2] == 128)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 0) & (label[:, :, 1] == 128) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 128) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 64) & (label[:, :, 1] == 0) & (label[:, :, 2] == 128)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 192) & (label[:, :, 1] == 0) & (label[:, :, 2] == 192)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 64) & (label[:, :, 1] == 64) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    # must perform this\n",
    "    return image, tf.transpose(labels, [1, 2, 0])\n",
    "\n",
    "\n",
    "def create_ds(batch_size, istrain=True, maximage=False):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    if istrain:\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_train/**/Images/*.png\"\n",
    "        images = glob.glob(directory, recursive=True)\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_train/**/Labels/*.png\"\n",
    "        labels = glob.glob(directory, recursive=True)\n",
    "    else:\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_val/**/Images/*.png\"\n",
    "        images = glob.glob(directory, recursive=True)\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_val/**/Labels/*.png\"\n",
    "        labels = glob.glob(directory, recursive=True)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    ds = ds.cache()\n",
    "    if istrain:\n",
    "        ds = ds.shuffle(6400, SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(get_image_decode, AUTOTUNE)\n",
    "\n",
    "    if not maximage:\n",
    "        ds = ds.flat_map(decode_crop)\n",
    "\n",
    "    ds = ds.map(get_mask, AUTOTUNE)\n",
    "\n",
    "    # batch and prefetch\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    if istrain:\n",
    "        ds = ds.prefetch(AUTOTUNE)\n",
    "\n",
    "    return ds\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# data = create_ds(1, False, True)\n",
    "\n",
    "# total_mask_sum = tf.zeros([8], tf.int64)\n",
    "# for image, mask in data:\n",
    "#     mask = tf.cast(mask, tf.int64)\n",
    "#     mask_sum = tf.reduce_sum(mask, [0, 1, 2])\n",
    "\n",
    "#     total_mask_sum += mask_sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.bar([str(i) for i in range(8)], total_mask_sum / tf.reduce_max(total_mask_sum), color=['blue', 'red', 'green', 'yellow'], align='center', alpha=0.8)\n",
    "# plt.grid(True)\n",
    "# plt.title('The number of samples mask in each class')\n",
    "\n",
    "# (total_mask_sum / tf.reduce_mean(total_mask_sum))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def create_backbone():\n",
    "    _backbone = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False, input_shape=[None, None, 3]\n",
    "    )\n",
    "\n",
    "    outputs = [\n",
    "        layer.output\n",
    "        for layer in _backbone.layers\n",
    "        if layer.name\n",
    "        in [\n",
    "            \"block2a_activation\",\n",
    "            \"block3a_activation\",\n",
    "            \"block5a_activation\",\n",
    "            \"block7a_activation\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[_backbone.input], outputs=outputs, name=\"efficientb0_backbone\"\n",
    "    )\n",
    "\n",
    "\n",
    "class FPN(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(name=\"Feature_Pyramid_Network\", **kwargs)\n",
    "\n",
    "        self.backbone = create_backbone()\n",
    "        self.conv5_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv4_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv3_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv2_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv5_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv5_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv5_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.conv4_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv4_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv4_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.upscale = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        # 112x112, 56x56, 28x28, 14x14\n",
    "        conv2, conv3, conv4, conv5 = self.backbone(images, training=training)\n",
    "        conv5_m = self.conv5_1x1(conv5)\n",
    "        conv5_p = self.conv5_3x3_1(conv5_m)\n",
    "        conv5_p = self.conv5_3x3_2(conv5_p)\n",
    "        conv5_p = self.conv5_bn(conv5_p, training=training)\n",
    "\n",
    "        conv4_m_1 = self.upscale(conv5_m)\n",
    "        conv4_m_2 = self.conv4_1x1(conv4)\n",
    "        conv4_m = conv4_m_1 + conv4_m_2\n",
    "        conv4_p = self.conv4_3x3_1(conv4_m)\n",
    "        conv4_p = self.conv4_3x3_2(conv4_p)\n",
    "        conv4_p = self.conv4_bn(conv4_p, training=training)\n",
    "\n",
    "        conv3_m_1 = self.upscale(conv4_m)\n",
    "        conv3_m_2 = self.conv3_1x1(conv3)\n",
    "        conv3_m = conv3_m_1 + conv3_m_2\n",
    "        conv3_p = self.conv3_3x3_1(conv3_m)\n",
    "        conv3_p = self.conv3_3x3_2(conv3_p)\n",
    "        conv3_p = self.conv3_bn(conv3_p, training=training)\n",
    "\n",
    "        conv2_m_1 = self.upscale(conv3_m)\n",
    "        conv2_m_2 = self.conv2_1x1(conv2)\n",
    "        conv2_m = conv2_m_1 + conv2_m_2\n",
    "        conv2_p = self.conv2_3x3_1(conv2_m)\n",
    "        conv2_p = self.conv2_3x3_2(conv2_p)\n",
    "        conv2_p = self.conv2_bn(conv2_p, training=training)\n",
    "        return conv5_p, conv4_p, conv3_p, conv2_p\n",
    "\n",
    "\n",
    "class FCN(tf.keras.Model):\n",
    "    def __init__(self, n_classes=8, **kwargs):\n",
    "        super().__init__(name=\"FCN\", **kwargs)\n",
    "        self.fpn = FPN()\n",
    "        self.upscale_2x = tf.keras.layers.UpSampling2D()\n",
    "        self.upscale_4x = tf.keras.layers.UpSampling2D((4, 4))\n",
    "        self.upscale_8x = tf.keras.layers.UpSampling2D((8, 8))\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.conv6 = tf.keras.layers.Conv2D(\n",
    "            filters=(512), kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.bnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.conv7 = tf.keras.layers.Conv2D(\n",
    "            filters=n_classes, kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale_final = tf.keras.layers.UpSampling2D(\n",
    "            size=(4, 4), interpolation=\"bilinear\"\n",
    "        )\n",
    "        self.final_activation = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def set_trainable(self, state=False):\n",
    "        for layer in self.fpn.backbone.layers:\n",
    "            layer.trainable = state\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        conv5_p, conv4_p, conv3_p, conv2_p = self.fpn(images, training=training)\n",
    "        m_5 = self.upscale_8x(conv5_p)\n",
    "        m_4 = self.upscale_4x(conv4_p)\n",
    "        m_3 = self.upscale_2x(conv3_p)\n",
    "        m_2 = conv2_p\n",
    "\n",
    "        m_all = self.concat([m_2, m_3, m_4, m_5])\n",
    "        m_all = self.conv6(m_all)\n",
    "        m_all = self.bnorm(m_all, training=training)\n",
    "        m_all = self.conv7(m_all)\n",
    "        m_all = self.upscale_final(m_all)\n",
    "        m_all = self.final_activation(m_all)\n",
    "\n",
    "        return m_all\n",
    "\n",
    "\n",
    "class FCN_ORIG(tf.keras.Model):\n",
    "    def __init__(self, n_classes=8, **kwargs):\n",
    "        super().__init__(name=\"FCN_ORIG\", **kwargs)\n",
    "\n",
    "        self.backbone = create_backbone()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale2x_1 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale2x_2 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale8x = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(16, 16),\n",
    "            strides=(8, 8),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.final_activation = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        _, conv1_o, conv2_o, conv3_o = self.backbone(images, training=training)\n",
    "        conv1_o = self.conv1(conv1_o)\n",
    "        conv2_o = self.conv2(conv2_o)\n",
    "        conv3_o = self.conv3(conv3_o)\n",
    "\n",
    "        fcn_16x = self.upscale2x_1(conv3_o) + conv2_o\n",
    "        fcn_8x = self.upscale2x_2(fcn_16x) + conv1_o\n",
    "        final_output = self.upscale8x(fcn_8x)\n",
    "        final_output = self.final_activation(final_output)\n",
    "        return final_output\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# the network must OUTPUT in logits [-inf, inf]\n",
    "def Jindex(target, pred):\n",
    "    intersection = tf.reduce_sum(target * pred, [0, 1, 2])\n",
    "    union = tf.reduce_sum(target + pred, [0, 1, 2]) - intersection\n",
    "    return tf.reduce_mean(intersection / union)\n",
    "\n",
    "\n",
    "# # I hypothesis from_logits is the cause of NaN\n",
    "def loss_fn(target, sample, weight, alpha=1, beta=0.5):\n",
    "    loss = alpha * tf.reduce_mean(\n",
    "        tf.keras.losses.categorical_crossentropy(target, sample),\n",
    "    )\n",
    "    return loss + beta * (1 - Jindex(target, sample, weight))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# this iteration is calculated fom 160 iteration from\n",
    "# paper\n",
    "n_epoch = 20\n",
    "n_classes = 8\n",
    "ds = create_ds(1)\n",
    "test = create_ds(1, False)\n",
    "# model = FCN(8)\n",
    "# model = sm.Unet(backbone_name='resnet50', encoder_weights='imagenet', encoder_freeze=False, activation='softmax', classes=8)\n",
    "model = FCN_ORIG(8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# lr = 0.001 is good but spiky, next learning rate to test is 0.0005\n",
    "\n",
    "# both fpn and unet uses 1e-4 learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(0.00002)\n",
    "dice_loss = sm.losses.CategoricalFocalLoss()\n",
    "focal_loss = sm.losses.DiceLoss()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "ckptmg = tf.train.CheckpointManager(ckpt, 'new_orig_fcn', 5)\n",
    "ckptmg.restore_or_initialize()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# normalize gradient for pretrained network\n",
    "iteration = 0\n",
    "\n",
    "# REMEMBER TO CHANGE THIS BACK to nepoch\n",
    "for _ in range(n_epoch):\n",
    "    total_loss = 0\n",
    "    total_num = 0\n",
    "    total_iou = 0\n",
    "    for bs_images, bs_labels in ds:\n",
    "        # REMEMBER HERE\n",
    "        # Comment this if you are training FPN and ORIG_FCN\n",
    "        # bs_images = sm.get_preprocessing('resnet')(bs_images)\n",
    "\n",
    "        with tf.GradientTape() as t:\n",
    "            output = model(bs_images)\n",
    "            c_loss = dice_loss(bs_labels, output) + 0.5 * focal_loss(bs_labels, output)\n",
    "\n",
    "        total_num += 1\n",
    "        total_loss += c_loss.numpy()\n",
    "        total_iou += sm.metrics.iou_score(bs_labels, output)\n",
    "        iteration += 1\n",
    "\n",
    "        grad = t.gradient(c_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(\"iou\", total_iou / total_num, step=iteration)\n",
    "            tf.summary.scalar(\"loss\", total_loss / total_num, step=iteration)\n",
    "\n",
    "\n",
    "    total_iou_test = 0\n",
    "    total_loss_test = 0\n",
    "    total_num_test = 0\n",
    "    for bs_images, bs_labels in test:\n",
    "        output = model(bs_images, training=False)\n",
    "\n",
    "        total_num_test += 1\n",
    "        total_loss_test += dice_loss(bs_labels, output) + 0.5 * focal_loss(\n",
    "            bs_labels, output\n",
    "        )\n",
    "        total_iou_test += sm.metrics.iou_score(bs_labels, output)\n",
    "\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"iou\", total_iou_test / total_num_test, step=iteration)\n",
    "        tf.summary.scalar(\"loss\", total_loss_test / total_num_test, step=iteration)\n",
    "\n",
    "    ckptmg.save()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# To explain about LABELS\n",
    "\n",
    "1. Background Clutter (0, 0, 0)\n",
    "2. Building           (128, 0, 0)\n",
    "3. Road               (128, 64, 128)\n",
    "4. Tree               (0, 128, 0)\n",
    "5. Low Vegetation     (128, 128, 0)\n",
    "6. Moving Car         (64, 0, 128)\n",
    "7. Static Car         (192, 0, 192)\n",
    "8. Human              (64, 64, 0)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "num = 0\n",
    "total_iou = 0\n",
    "classes_iou = tf.zeros([8])\n",
    "\n",
    "test = create_ds(1, False, True)\n",
    "\n",
    "for d, y in test.take(1):\n",
    "    # add 8 padding horizontally\n",
    "    d = sm.get_preprocessing('resnet50')(d)\n",
    "    out_img = model(tf.pad(d, [[0, 0], [8, 8], [0, 0], [0,0]]), training=False)\n",
    "    y = tf.pad(y, [[0, 0], [8, 8], [0, 0], [0,0]])\n",
    "\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(20, 20))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.imshow(y[0, :, :, i])\n",
    "\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(20, 20))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.imshow(out_img[0, :, :, i])\n",
    "\n",
    "    num += 1\n",
    "    # total_iou += sm.metrics.iou_score(y, out_img).numpy()\n",
    "    intersection = tf.reduce_sum(out_img * y, [0, 1, 2])\n",
    "    union = tf.reduce_sum(out_img + y, [0, 1, 2]) - intersection\n",
    "    classes_iou += intersection / union\n",
    "\n",
    "print(classes_iou / num)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1,32,1088,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ad63b6978ae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# add 8 padding horizontally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet50'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mout_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ba27a6b58774>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, images, training)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv1_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv3_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mconv1_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1_o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mconv2_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2_o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \"\"\"\n\u001b[0;32m--> 420\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    421\u001b[0m         inputs, training=training, mask=mask)\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m     name=None):\n\u001b[0;32m-> 1012\u001b[0;31m   return convolution_internal(\n\u001b[0m\u001b[1;32m   1013\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m       return op(\n\u001b[0m\u001b[1;32m   1143\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m           \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2594\u001b[0m     \u001b[0;31m# We avoid calling squeeze_batch_dims to reduce extra python function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m     \u001b[0;31m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2596\u001b[0;31m     return gen_nn_ops.conv2d(\n\u001b[0m\u001b[1;32m   2597\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2598\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    930\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf21/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,32,1088,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# result for FPN\n",
    "result_fpn = [\n",
    "    0.46724114,\n",
    "    0.8189325,\n",
    "    0.6044639,\n",
    "    0.7086379,\n",
    "    0.45720816,\n",
    "    0.34781703,\n",
    "    0.42935002,\n",
    "    0.24366052,\n",
    "]\n",
    "\n",
    "# result for UNET\n",
    "result_unet = [\n",
    "    3.3367285e-01,\n",
    "    6.6008663e-01,\n",
    "    3.7074447e-01,\n",
    "    5.8561021e-01,\n",
    "    4.3773600e-01,\n",
    "    0.0000000e00,\n",
    "    7.9385777e-20,\n",
    "    1.1507687e-23,\n",
    "]\n",
    "\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    {\"fpn\": result_fpn, \"unet\": result_unet},\n",
    ")\n",
    "\n",
    "results = results.melt()\n",
    "results[\"index\"] = [i % 8 for i in range(16)]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"IoU of FPN vs UNET\")\n",
    "sb.barplot(data=results, x=\"index\", y=\"value\", hue=\"variable\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# iou loss\n",
    "intersection = tf.reduce_sum(out_img * y, [0, 1, 2])\n",
    "union = tf.reduce_sum(out_img + y, [0, 1, 2]) - intersection\n",
    "# print(tf.reduce_mean(intersection / union))\n",
    "(intersection / union).numpy()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.5063492 , 0.83595437, 0.40129408, 0.7200879 , 0.5190978 ,\n",
       "       0.        , 0.2961473 , 0.07226681], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dice loss\n",
    "intersection = tf.reduce_sum(2 * out_img * y, [0, 1, 2])\n",
    "union = tf.reduce_sum(out_img + y, [0, 1, 2])\n",
    "print(tf.reduce_mean(intersection / union))\n",
    "intersection / union"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0.53351736, shape=(), dtype=float32)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       "array([0.6722866 , 0.9106483 , 0.5727478 , 0.83726865, 0.68342906,\n",
       "       0.        , 0.4569655 , 0.13479258], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def to_save(images, name):\n",
    "    for idx in range(images[0].shape[-1]):\n",
    "        byte = tf.image.encode_jpeg(tf.image.convert_image_dtype(images[0, ..., idx][..., tf.newaxis], tf.uint8))\n",
    "        tf.io.write_file(f'{name}_{idx}_image.jpeg', byte)\n",
    "\n",
    "to_save(out_img, 'test')\n",
    "# to_save(y, 'target')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf21': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "e46bccaa29de72a71dcf898a5d29186cf71d4697eff3ce343101d8011b0a26e8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64c643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816d00ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import segmentation_models as sm\n",
    "import functools as ft\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "sm.set_framework(\"tf.keras\")\n",
    "sm.framework()\n",
    "\n",
    "tf.random.set_seed(1024)\n",
    "SEED = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b2dae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seagull_path(istrain=True):\n",
    "    directory = \"C:\\Alans\\seagull\"\n",
    "\n",
    "    if istrain:\n",
    "        trainimg = os.path.join(directory, \"trainimg\", \"*.jpg\")\n",
    "        images = glob.glob(trainimg, recursive=True)\n",
    "        trainmask = os.path.join(directory, \"trainmask\", \"*.jpg\")\n",
    "        labels = glob.glob(trainmask, recursive=True)\n",
    "    else:\n",
    "        testimg = os.path.join(directory, \"testimg\", \"*.jpg\")\n",
    "        images = glob.glob(testimg, recursive=True)\n",
    "        testmask = os.path.join(directory, \"testmask\", \"*.jpg\")\n",
    "        labels = glob.glob(testmask, recursive=True)\n",
    "\n",
    "    print(len(images), len(labels))\n",
    "\n",
    "    mask_set = set()\n",
    "    image_set = set()\n",
    "    for lbl in labels:\n",
    "        lbl = lbl.split('\\\\')[-1]\n",
    "        mask_set.add(lbl)\n",
    "\n",
    "    for img in images:\n",
    "        img = img.split('\\\\')[-1]\n",
    "        image_set.add(img)\n",
    "\n",
    "    complete_path = mask_set.intersection(image_set)\n",
    "    print(\n",
    "        f\"IMG - LBL NUM: {len(image_set.difference(mask_set))}, Intersection: {len(complete_path)}\"\n",
    "    )\n",
    "\n",
    "    return [i for i in complete_path]\n",
    "\n",
    "def get_image_decode(image, label):\n",
    "    image = tf.io.read_file(image, \"image\")\n",
    "    label = tf.io.read_file(label, \"label\")\n",
    "\n",
    "    image = tf.image.decode_image(image)\n",
    "    label = tf.image.decode_image(label)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def path_2_test(path):\n",
    "    return (\n",
    "        r\"C:\\Alans\\seagull\\\\testimg\\\\\" + path,\n",
    "        r\"C:\\Alans\\seagull\\\\testmask\\\\\" + path,\n",
    "    )\n",
    "\n",
    "\n",
    "def path_2_train(path):\n",
    "    return (\n",
    "        r\"C:\\Alans\\seagull\\trainimg\\\\\" + path,\n",
    "        r\"C:\\Alans\\seagull\\trainmask\\\\\" + path,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_mask(image, label):\n",
    "    labels = []\n",
    "    labels.append(label[:, :, 0] == 0)\n",
    "    labels.append(label[:, :, 0] == 255)\n",
    "\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    # must perform this\n",
    "    return image, tf.transpose(labels, [1, 2, 0])\n",
    "\n",
    "\n",
    "def create_ds(batch_size, ratio=0.8):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    paths = get_seagull_path()\n",
    "    ds1 = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    ds1 = ds1.map(path_2_train, AUTOTUNE)\n",
    "\n",
    "    paths = get_seagull_path(False)\n",
    "    ds2 = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    ds2 = ds2.map(path_2_test, AUTOTUNE)\n",
    "\n",
    "    ds = ds1.concatenate(ds2)\n",
    "    ds = ds.cache()\n",
    "\n",
    "    takefortrain = int(23124 * ratio)\n",
    "    trainds = ds.take(takefortrain)\n",
    "    testds = ds.skip(takefortrain).take(23124 - takefortrain)\n",
    "\n",
    "    trainds = trainds.shuffle(23124)\n",
    "\n",
    "    trainds = trainds.map(get_image_decode, AUTOTUNE)\n",
    "    trainds = trainds.map(get_mask, AUTOTUNE)\n",
    "    testds = testds.map(get_image_decode, AUTOTUNE)\n",
    "    testds = testds.map(get_mask, AUTOTUNE)\n",
    "\n",
    "    # # batch and prefetch\n",
    "    trainds = trainds.batch(batch_size)\n",
    "    testds = testds.batch(batch_size)\n",
    "\n",
    "    trainds = trainds.prefetch(AUTOTUNE)\n",
    "\n",
    "    return trainds, testds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0bc5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_backbone_efficient():\n",
    "    _backbone = keras.applications.EfficientNetB0(include_top=False)\n",
    "\n",
    "    outputs = [\n",
    "        layer.output\n",
    "        for layer in _backbone.layers\n",
    "        if layer.name\n",
    "        in [\n",
    "            \"block2a_activation\",\n",
    "            \"block3a_activation\",\n",
    "            \"block5a_activation\",\n",
    "            \"block7a_activation\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[_backbone.input], outputs=outputs, name=\"efficientb0_backbone\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ef5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(tf.keras.layers.Layer):\n",
    "    def __init__(self, backbone=None, **kwargs):\n",
    "        super().__init__(name=\"Feature_Pyramid_Network\", **kwargs)\n",
    "\n",
    "        self.backbone = create_backbone_efficient()\n",
    "\n",
    "        self.conv5_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv4_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv3_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv2_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv5_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv5_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv4_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv4_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        # 112x112, 56x56, 28x28, 14x14\n",
    "        conv2, conv3, conv4, conv5 = self.backbone(images, training=False)\n",
    "        conv5_m = self.conv5_1x1(conv5)\n",
    "        conv5_p = self.conv5_3x3_1(conv5_m)\n",
    "        conv5_p = self.conv5_3x3_2(conv5_p)\n",
    "\n",
    "        conv4_m_1 = self.upscale(conv5_m)\n",
    "        conv4_m_2 = self.conv4_1x1(conv4)\n",
    "        conv4_m = conv4_m_1 + conv4_m_2\n",
    "        conv4_p = self.conv4_3x3_1(conv4_m)\n",
    "        conv4_p = self.conv4_3x3_2(conv4_p)\n",
    "\n",
    "        conv3_m_1 = self.upscale(conv4_m)\n",
    "        conv3_m_2 = self.conv3_1x1(conv3)\n",
    "        conv3_m = conv3_m_1 + conv3_m_2\n",
    "        conv3_p = self.conv3_3x3_1(conv3_m)\n",
    "        conv3_p = self.conv3_3x3_2(conv3_p)\n",
    "\n",
    "        conv2_m_1 = self.upscale(conv3_m)\n",
    "        conv2_m_2 = self.conv2_1x1(conv2)\n",
    "        conv2_m = conv2_m_1 + conv2_m_2\n",
    "        conv2_p = self.conv2_3x3_1(conv2_m)\n",
    "        conv2_p = self.conv2_3x3_2(conv2_p)\n",
    "\n",
    "        return conv5_p, conv4_p, conv3_p, conv2_p\n",
    "\n",
    "\n",
    "class FCN(tf.keras.Model):\n",
    "    def __init__(self, n_classes=8, backbone=None, **kwargs):\n",
    "        super().__init__(name=\"FCN\", **kwargs)\n",
    "        self.fpn = FPN(backbone)\n",
    "        self.upscale_2x = tf.keras.layers.UpSampling2D()\n",
    "        self.upscale_4x = tf.keras.layers.UpSampling2D((4, 4))\n",
    "        self.upscale_8x = tf.keras.layers.UpSampling2D((8, 8))\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.conv6 = tf.keras.layers.Conv2D(\n",
    "            filters=(512), kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv7 = tf.keras.layers.Conv2D(\n",
    "            filters=n_classes, kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale_final = tf.keras.layers.UpSampling2D(\n",
    "            size=(4, 4), interpolation=\"bilinear\"\n",
    "        )\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        conv5_p, conv4_p, conv3_p, conv2_p = self.fpn(images, training=training)\n",
    "        m_5 = self.upscale_8x(conv5_p)\n",
    "        m_4 = self.upscale_4x(conv4_p)\n",
    "        m_3 = self.upscale_2x(conv3_p)\n",
    "        m_2 = conv2_p\n",
    "\n",
    "        m_all = self.concat([m_2, m_3, m_4, m_5])\n",
    "        m_all = self.conv6(m_all)\n",
    "        m_all = self.conv7(m_all)\n",
    "        m_all = self.upscale_final(m_all)\n",
    "\n",
    "        return m_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebebd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_ORIG(tf.keras.Model):\n",
    "    def __init__(self, n_classes=8, backbone=None, **kwargs):\n",
    "        super().__init__(name=\"FCN_ORIG\", **kwargs)\n",
    "\n",
    "        self.backbone = create_backbone_efficient()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale2x_1 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale2x_2 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale2x_3 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale2x_4 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(4, 4),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        conv1_o, conv2_o, conv3_o, conv4_o = self.backbone(images, training=False)\n",
    "        conv1_o = self.conv1(conv1_o)\n",
    "        conv2_o = self.conv2(conv2_o)\n",
    "        conv3_o = self.conv3(conv3_o)\n",
    "\n",
    "        fcn_16x = self.upscale2x_1(conv4_o) + conv3_o\n",
    "        fcn_8x = self.upscale2x_2(fcn_16x) + conv2_o\n",
    "        fcn_4x = self.upscale2x_3(fcn_8x) + conv1_o\n",
    "        final_output = self.upscale2x_4(fcn_4x)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcea6318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescalingUnet(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(RescalingUnet, self).__init__()\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return ((inputs * (1 / 255.0)) - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76060da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_model_unetfpn(mode=\"multiply\", n_classes=8):\n",
    "    model_unet = sm.Unet(\n",
    "        backbone_name=\"efficientnetb0\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        encoder_freeze=False,\n",
    "        classes=n_classes,\n",
    "        decoder_use_batchnorm=False,\n",
    "    )\n",
    "    model_fpn = FCN(n_classes)\n",
    "    conv1x1 = keras.layers.Conv2D(n_classes, 1, padding=\"same\", activation=\"softmax\")\n",
    "    input_layer = keras.layers.Input([None, None, 3])\n",
    "    rescale_layer = RescalingUnet()\n",
    "\n",
    "    if mode == \"concat\":\n",
    "        concat = keras.layers.Concatenate()\n",
    "\n",
    "    output_model_fcn = model_unet(rescale_layer(input_layer))\n",
    "    output_model_fpn = model_fpn(input_layer)\n",
    "\n",
    "    if mode == \"multiply\":\n",
    "        output = output_model_fcn * output_model_fpn\n",
    "    elif mode == \"sum\":\n",
    "        output = output_model_fcn + output_model_fpn\n",
    "    elif mode == \"concat\":\n",
    "        output = concat([output_model_fcn, output_model_fpn])\n",
    "    else:\n",
    "        raise AssertionError(\"mode selected is not in the list\")\n",
    "    output_final = conv1x1(output)\n",
    "    return keras.Model([input_layer], [output_final])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8de64f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_model_fcnfpn(mode=\"multi\", n_classes=8):\n",
    "    model_fcn = FCN_ORIG(n_classes)\n",
    "    model_fpn = FCN(n_classes)\n",
    "\n",
    "    conv1x1 = keras.layers.Conv2D(n_classes, 1, padding=\"same\", activation=\"softmax\")\n",
    "\n",
    "    input_layer = keras.layers.Input([None, None, 3])\n",
    "    output_model_fcn = model_fcn(input_layer)\n",
    "    output_model_fpn = model_fpn(input_layer)\n",
    "    output = output_model_fcn * output_model_fpn\n",
    "    output_final = conv1x1(output)\n",
    "\n",
    "    return keras.Model([input_layer], [output_final])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c49bb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network must OUTPUT in logits [-inf, inf]\n",
    "# make sure input dimension is [B, H, W, C]\n",
    "def Jindex(target, pred, showPerChannel=False):\n",
    "    intersection = tf.reduce_sum(target * pred, [0, 1, 2])\n",
    "    union = tf.reduce_sum(target + pred, [0, 1, 2]) - intersection\n",
    "    if showPerChannel:\n",
    "        return (intersection + 0.1) / (union + 0.1)\n",
    "    return tf.reduce_mean((intersection + 0.1) / (union + 0.1))\n",
    "\n",
    "\n",
    "def Dice(target, pred):\n",
    "    intersection = tf.reduce_sum(2 * pred * target, [0, 1, 2])\n",
    "    union = tf.reduce_sum(pred + target, [0, 1, 2])\n",
    "    return tf.reduce_mean((intersection + 0.1) / (union + 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ca70ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, trainds, testds):\n",
    "    # evaluate on training set\n",
    "    iteration = 0\n",
    "    iou = tf.zeros([2])\n",
    "    for bs_images, bs_label in trainds:\n",
    "        output = model(bs_images, training=False)\n",
    "        iou += Jindex(bs_label, output, showPerChannel=True)\n",
    "        iteration += 1\n",
    "    train_iou = iou / iteration\n",
    "\n",
    "    # evaluate on test set\n",
    "    iteration = 0\n",
    "    iou = tf.zeros([2])\n",
    "    for bs_images, bs_label in testds:\n",
    "        output = model(bs_images, training=False)\n",
    "        iou += Jindex(bs_label, output, showPerChannel=True)\n",
    "        iteration += 1\n",
    "    test_iou = iou / iteration\n",
    "\n",
    "    print(f\"Train IoU: {train_iou} | Train IoU: {test_iou}\")\n",
    "\n",
    "    return train_iou, test_iou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca707f",
   "metadata": {},
   "source": [
    "# Testing starts below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4999a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19373 17243\n",
      "IMG - LBL NUM: 10687, Intersection: 8686\n",
      "14476 14443\n",
      "IMG - LBL NUM: 38, Intersection: 14438\n"
     ]
    }
   ],
   "source": [
    "n_classes = 2\n",
    "batch_size = 32\n",
    "trainds, testds = create_ds(batch_size)\n",
    "iou_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "948f2551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IoU: [0.9940915  0.00102931] | Train IoU: [0.9926623  0.00128302]\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# UNET(CONCAT) + FPN\n",
    "####################################\n",
    "name = \"unetfpnconcat\"\n",
    "model = combined_model_unetfpn(\"concat\", n_classes=2)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckptmg = tf.train.CheckpointManager(ckpt, f\"trained_model_seagull/unetfpnconcat\", 5)\n",
    "ckptmg.restore_or_initialize()\n",
    "\n",
    "train_iou, test_iou = eval_model(model, trainds, testds)\n",
    "iou_list.append([name, train_iou, test_iou])\n",
    "keras.backend.clear_session()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203f93de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    return np.concatenate([[x[0]], x[1].numpy(), x[2].numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05ebb407",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "17 columns passed, passed data had 5 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Alans\\Miniconda\\envs\\tf\\lib\\site-packages\\pandas\\core\\internals\\construction.py:982\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 982\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    983\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    984\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Alans\\Miniconda\\envs\\tf\\lib\\site-packages\\pandas\\core\\internals\\construction.py:1030\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi_list \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(content):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[39m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1030\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m   1031\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(columns)\u001b[39m}\u001b[39;00m\u001b[39m columns passed, passed data had \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1032\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(content)\u001b[39m}\u001b[39;00m\u001b[39m columns\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m     )\n\u001b[0;32m   1034\u001b[0m \u001b[39melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1035\u001b[0m \n\u001b[0;32m   1036\u001b[0m     \u001b[39m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 17 columns passed, passed data had 5 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DR AZIZI\\Documents\\ALANS\\project_multi_model_testing_seagull.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=1'>2</a>\u001b[0m     \u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(convert, iou_list)),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=2'>3</a>\u001b[0m     columns\u001b[39m=\u001b[39;49m[\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=3'>4</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=4'>5</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainc1\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainc2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainc3\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainc4\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainc5\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=9'>10</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainc6\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=10'>11</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainc7\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=11'>12</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainc8\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=12'>13</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtestc1\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=13'>14</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtestc2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=14'>15</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtestc3\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=15'>16</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtestc4\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=16'>17</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtestc5\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=17'>18</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtestc6\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=18'>19</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtestc7\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=19'>20</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtestc8\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=20'>21</a>\u001b[0m     ],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=21'>22</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=23'>24</a>\u001b[0m results\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mresults_multiple_seagull.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DR%20AZIZI/Documents/ALANS/project_multi_model_testing_seagull.ipynb#ch0000019?line=24'>25</a>\u001b[0m second \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Alans\\Miniconda\\envs\\tf\\lib\\site-packages\\pandas\\core\\frame.py:721\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m         \u001b[39m# error: Argument 1 to \"ensure_index\" has incompatible type\u001b[39;00m\n\u001b[0;32m    718\u001b[0m         \u001b[39m# \"Collection[Any]\"; expected \"Union[Union[Union[ExtensionArray,\u001b[39;00m\n\u001b[0;32m    719\u001b[0m         \u001b[39m# ndarray], Index, Series], Sequence[Any]]\"\u001b[39;00m\n\u001b[0;32m    720\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    722\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    723\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    724\u001b[0m         data,\n\u001b[0;32m    725\u001b[0m         columns,\n\u001b[0;32m    726\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    727\u001b[0m         dtype,\n\u001b[0;32m    728\u001b[0m     )\n\u001b[0;32m    729\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    730\u001b[0m         arrays,\n\u001b[0;32m    731\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    734\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    735\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Alans\\Miniconda\\envs\\tf\\lib\\site-packages\\pandas\\core\\internals\\construction.py:519\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[1;32m--> 519\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    520\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    522\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Alans\\Miniconda\\envs\\tf\\lib\\site-packages\\pandas\\core\\internals\\construction.py:883\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    880\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[0;32m    881\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 883\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    884\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mc:\\Alans\\Miniconda\\envs\\tf\\lib\\site-packages\\pandas\\core\\internals\\construction.py:985\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    982\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    983\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    984\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[0;32m    988\u001b[0m     contents \u001b[39m=\u001b[39m _convert_object_array(contents, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 17 columns passed, passed data had 5 columns"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(\n",
    "    list(map(convert, iou_list)),\n",
    "    columns=[\n",
    "        \"model\",\n",
    "        \"trainc1\",\n",
    "        \"trainc2\",\n",
    "        \"testc1\",\n",
    "        \"testc2\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "results.to_csv('results_multiple_seagull.csv')\n",
    "second = results.pop(\"model\")\n",
    "\n",
    "results.iloc[:, 0:] = results.iloc[:, 0:].astype(\"float\")\n",
    "results[\"model\"] = second\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54802ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots(figsize=(15, 7))\n",
    "sb.barplot(\n",
    "    data=results.melt(id_vars=\"model\").drop_duplicates(),\n",
    "    y=\"value\",\n",
    "    x=\"variable\",\n",
    "    hue=\"model\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"class\")\n",
    "ax.set_ylabel(\"IoU\")\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f009644687fbff803937e34e13187dc56579602643eee9f001c34fd1c6bb4d9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

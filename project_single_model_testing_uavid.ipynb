{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64c643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448e19e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import segmentation_models as sm\n",
    "import functools as ft\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import time\n",
    "\n",
    "sm.set_framework(\"tf.keras\")\n",
    "sm.framework()\n",
    "\n",
    "tf.random.set_seed(1024)\n",
    "SEED = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77180702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_decode(image, label):\n",
    "    image = tf.io.read_file(image, \"image\")\n",
    "    label = tf.io.read_file(label, \"label\")\n",
    "\n",
    "    image = tf.image.decode_image(image)\n",
    "    label = tf.image.decode_image(label)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# [w, h, c], 448, 448, 3\n",
    "def decode_crop(image, label):\n",
    "    image = image[368 // 2 : -(368 // 2), 256 // 2 : -(256 // 2)]\n",
    "    label = label[368 // 2 : -(368 // 2), 256 // 2 : -(256 // 2)]\n",
    "\n",
    "    img_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for index in range(4 * 8):\n",
    "        x, y = index // 8, index % 8\n",
    "        img_array.append(image[448 * x : 448 * (1 + x), 448 * y : 448 * (1 + y)])\n",
    "        label_array.append(label[448 * x : 448 * (1 + x), 448 * y : 448 * (1 + y)])\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((img_array, label_array))\n",
    "\n",
    "\n",
    "def get_mask(image, label):\n",
    "    labels = []\n",
    "    labels.append((label[:, :, 0] == 0) & (label[:, :, 1] == 0) & (label[:, :, 2] == 0))\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 0) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 64) & (label[:, :, 2] == 128)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 0) & (label[:, :, 1] == 128) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 128) & (label[:, :, 1] == 128) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 64) & (label[:, :, 1] == 0) & (label[:, :, 2] == 128)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 192) & (label[:, :, 1] == 0) & (label[:, :, 2] == 192)\n",
    "    )\n",
    "    labels.append(\n",
    "        (label[:, :, 0] == 64) & (label[:, :, 1] == 64) & (label[:, :, 2] == 0)\n",
    "    )\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    # must perform this\n",
    "    return image, tf.transpose(labels, [1, 2, 0])\n",
    "\n",
    "\n",
    "def create_ds(batch_size, istrain=True, maximage=False):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    if istrain:\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_train/**/Images/*.png\"\n",
    "        images = glob.glob(directory, recursive=True)\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_train/**/Labels/*.png\"\n",
    "        labels = glob.glob(directory, recursive=True)\n",
    "    else:\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_val/**/Images/*.png\"\n",
    "        images = glob.glob(directory, recursive=True)\n",
    "        directory = \"/home/hackerton/Downloads/uavid_v1.5_official_release/uavid_val/**/Labels/*.png\"\n",
    "        labels = glob.glob(directory, recursive=True)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    ds = ds.cache()\n",
    "    if istrain:\n",
    "        ds = ds.shuffle(6400, SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(get_image_decode, AUTOTUNE)\n",
    "\n",
    "    if not maximage:\n",
    "        ds = ds.flat_map(decode_crop)\n",
    "\n",
    "    ds = ds.map(get_mask, AUTOTUNE)\n",
    "\n",
    "    # batch and prefetch\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    if istrain:\n",
    "        ds = ds.prefetch(AUTOTUNE)\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0bc5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_backbone_efficient():\n",
    "    _backbone = keras.applications.EfficientNetB0(include_top=False)\n",
    "\n",
    "    outputs = [\n",
    "        layer.output\n",
    "        for layer in _backbone.layers\n",
    "        if layer.name\n",
    "        in [\n",
    "            \"block2a_activation\",\n",
    "            \"block3a_activation\",\n",
    "            \"block5a_activation\",\n",
    "            \"block7a_activation\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[_backbone.input], outputs=outputs, name=\"efficientb0_backbone\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ef5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(tf.keras.layers.Layer):\n",
    "    def __init__(self, backbone=None, **kwargs):\n",
    "        super().__init__(name=\"Feature_Pyramid_Network\", **kwargs)\n",
    "\n",
    "        self.backbone = create_backbone_efficient()\n",
    "\n",
    "        self.conv5_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv4_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv3_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv2_1x1 = tf.keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(1, 1), padding=\"same\"\n",
    "        )\n",
    "        self.conv5_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv5_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv4_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv4_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2_3x3_1 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2_3x3_2 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        # 112x112, 56x56, 28x28, 14x14\n",
    "        conv2, conv3, conv4, conv5 = self.backbone(images, training=False)\n",
    "        conv5_m = self.conv5_1x1(conv5)\n",
    "        conv5_p = self.conv5_3x3_1(conv5_m)\n",
    "        conv5_p = self.conv5_3x3_2(conv5_p)\n",
    "\n",
    "        conv4_m_1 = self.upscale(conv5_m)\n",
    "        conv4_m_2 = self.conv4_1x1(conv4)\n",
    "        conv4_m = conv4_m_1 + conv4_m_2\n",
    "        conv4_p = self.conv4_3x3_1(conv4_m)\n",
    "        conv4_p = self.conv4_3x3_2(conv4_p)\n",
    "\n",
    "        conv3_m_1 = self.upscale(conv4_m)\n",
    "        conv3_m_2 = self.conv3_1x1(conv3)\n",
    "        conv3_m = conv3_m_1 + conv3_m_2\n",
    "        conv3_p = self.conv3_3x3_1(conv3_m)\n",
    "        conv3_p = self.conv3_3x3_2(conv3_p)\n",
    "\n",
    "        conv2_m_1 = self.upscale(conv3_m)\n",
    "        conv2_m_2 = self.conv2_1x1(conv2)\n",
    "        conv2_m = conv2_m_1 + conv2_m_2\n",
    "        conv2_p = self.conv2_3x3_1(conv2_m)\n",
    "        conv2_p = self.conv2_3x3_2(conv2_p)\n",
    "\n",
    "        return conv5_p, conv4_p, conv3_p, conv2_p\n",
    "\n",
    "\n",
    "class FCN(tf.keras.Model):\n",
    "    def __init__(self, n_classes=8, backbone=None, **kwargs):\n",
    "        super().__init__(name=\"FCN\", **kwargs)\n",
    "        self.fpn = FPN(backbone)\n",
    "        self.upscale_2x = tf.keras.layers.UpSampling2D()\n",
    "        self.upscale_4x = tf.keras.layers.UpSampling2D((4, 4))\n",
    "        self.upscale_8x = tf.keras.layers.UpSampling2D((8, 8))\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.conv6 = tf.keras.layers.Conv2D(\n",
    "            filters=(512), kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv7 = tf.keras.layers.Conv2D(\n",
    "            filters=n_classes, kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale_final = tf.keras.layers.UpSampling2D(\n",
    "            size=(4, 4), interpolation=\"bilinear\"\n",
    "        )\n",
    "        self.final_activation = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        conv5_p, conv4_p, conv3_p, conv2_p = self.fpn(images, training=training)\n",
    "        m_5 = self.upscale_8x(conv5_p)\n",
    "        m_4 = self.upscale_4x(conv4_p)\n",
    "        m_3 = self.upscale_2x(conv3_p)\n",
    "        m_2 = conv2_p\n",
    "\n",
    "        m_all = self.concat([m_2, m_3, m_4, m_5])\n",
    "        m_all = self.conv6(m_all)\n",
    "        m_all = self.conv7(m_all)\n",
    "        m_all = self.upscale_final(m_all)\n",
    "        m_all = self.final_activation(m_all)\n",
    "\n",
    "        return m_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebebd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_ORIG(tf.keras.Model):\n",
    "    def __init__(self, n_classes=8, backbone=None, **kwargs):\n",
    "        super().__init__(name=\"FCN_ORIG\", **kwargs)\n",
    "\n",
    "        self.backbone = create_backbone_efficient()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filters=(n_classes), kernel_size=(1, 1), padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.upscale2x_1 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale2x_2 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale2x_3 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.upscale2x_4 = tf.keras.layers.Convolution2DTranspose(\n",
    "            filters=8,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(4, 4),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.final_activation = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        conv1_o, conv2_o, conv3_o, conv4_o = self.backbone(images, training=False)\n",
    "        conv1_o = self.conv1(conv1_o)\n",
    "        conv2_o = self.conv2(conv2_o)\n",
    "        conv3_o = self.conv3(conv3_o)\n",
    "\n",
    "        fcn_16x = self.upscale2x_1(conv4_o) + conv3_o\n",
    "        fcn_8x = self.upscale2x_2(fcn_16x) + conv2_o\n",
    "        fcn_4x = self.upscale2x_3(fcn_8x) + conv1_o\n",
    "        final_output = self.upscale2x_4(fcn_4x)\n",
    "        final_output = self.final_activation(final_output)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6385667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network must OUTPUT in logits [-inf, inf]\n",
    "# make sure input dimension is [B, H, W, C]\n",
    "def Jindex(target, pred):\n",
    "    intersection = tf.reduce_sum(target * pred, [0, 1, 2])\n",
    "    union = tf.reduce_sum(target + pred, [0, 1, 2]) - intersection\n",
    "    return tf.reduce_mean((intersection + 0.1) / (union + 0.1))\n",
    "\n",
    "\n",
    "def Dice(target, pred):\n",
    "    intersection = tf.reduce_sum(2 * pred * target, [0, 1, 2])\n",
    "    union = tf.reduce_sum(pred + target, [0, 1, 2])\n",
    "    return tf.reduce_mean((intersection + 0.1) / (union + 0.1))\n",
    "\n",
    "def Jindex_class(target, pred):\n",
    "    intersection = tf.reduce_sum(target * pred, [0, 1, 2])\n",
    "    union = tf.reduce_sum(target + pred, [0, 1, 2]) - intersection\n",
    "    return (intersection + 0.1) / (union + 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39d00152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# lr = 0.001 is good but spiky, next learning rate to test is 0.0005\n",
    "# both fpn and unet uses 1e-4 learning rate\n",
    "\n",
    "# test_fpn, lr = 0.00001 (1e-4)\n",
    "# lr=1e-6, slow\n",
    "# lr=1e-5, fast\n",
    "# lr=5e-5, can trained but stagnate at 12k with iou of 0.69\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e43513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 00:10:23.219606: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-06-13 00:10:23.219645: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n",
      "2022-06-13 00:10:23.221865: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trained_model/unet/ckpt-20'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this iteration is calculated fom 160 iteration from\n",
    "# paper\n",
    "\n",
    "model_selection = [\"fcn8s\", \"unet\", \"fpn\"]\n",
    "model_choice = 1\n",
    "name_model = model_selection[model_choice]\n",
    "\n",
    "n_classes = 8\n",
    "batch_size = 8\n",
    "trainds = create_ds(batch_size)\n",
    "testds = create_ds(batch_size, False)\n",
    "\n",
    "if model_choice == 0:\n",
    "    model = FCN_ORIG(n_classes)\n",
    "elif model_choice == 1:\n",
    "    model = sm.Unet(\n",
    "        backbone_name=\"efficientnetb0\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        encoder_freeze=False,\n",
    "        activation=\"softmax\",\n",
    "        classes=n_classes,\n",
    "        decoder_use_batchnorm=False,\n",
    "    )\n",
    "elif model_choice == 2:\n",
    "    model = FCN(8)\n",
    "else:\n",
    "    assert \"No model chosen\"\n",
    "\n",
    "\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "dice_loss = sm.losses.DiceLoss()\n",
    "\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckptmg = tf.train.CheckpointManager(ckpt, f\"trained_model/{name_model}\", 5)\n",
    "ckptmg.restore_or_initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d66a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set...\n",
      "Time taken: 2360.636s\n",
      "IoU: 0.543\n",
      "Testing set...\n",
      "Time taken: 793.731s\n",
      "IoU: 0.444\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 1.0\n",
    "\n",
    "# testing on train dataset and testing dataset\n",
    "iou_train = np.zeros([800, 8])\n",
    "iou_test = np.zeros([280, 8])\n",
    "\n",
    "# training set\n",
    "print('Training set...')\n",
    "time_init = time.time()\n",
    "for idx, (bs_images, bs_labels) in trainds.enumerate():\n",
    "    if model_choice == 1:\n",
    "      bs_images = sm.get_preprocessing(\"efficientnetb0\")(bs_images)\n",
    "\n",
    "    if model_choice == 1:\n",
    "      output = model(bs_images, training=True)\n",
    "    else:\n",
    "      output = model(bs_images, training=False)\n",
    "    \n",
    "    jindex = Jindex_class(bs_labels, output)\n",
    "    iou_train[idx] = jindex\n",
    "time_taken = time.time() - time_init\n",
    "print(f'Time taken: {round(time_taken, 3)}s')\n",
    "print(f'IoU: {round(np.average(iou_train), 3)}')\n",
    "\n",
    "# testing set\n",
    "print('Testing set...')\n",
    "time_init = time.time()\n",
    "for idx, (bs_images, bs_labels) in testds.enumerate():\n",
    "    if model_choice == 1:\n",
    "      bs_images = sm.get_preprocessing(\"efficientnetb0\")(bs_images)\n",
    "\n",
    "    if model_choice == 1:\n",
    "      output = model(bs_images, training=True)\n",
    "    else:\n",
    "      output = model(bs_images, training=False)\n",
    "      \n",
    "    jindex = Jindex_class(bs_labels, output)\n",
    "    iou_test[idx] = jindex\n",
    "time_taken = time.time() - time_init\n",
    "print(f'Time taken: {round(time_taken, 3)}s')\n",
    "print(f'IoU: {round(np.average(iou_test), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 1 artists>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMVUlEQVR4nO3df6jd913H8edrCZlzLR3aq7gkXcIWGaEOrddsCrqp3UgVEmFTUhisOI1TwybzDwNKhUzQtjDxj/zRoIWhlKwWlDuXLoy5/TGxM7db3ExL8JJVkwju9gfV2a1d3Ns/7mk53t4f3yTn3pu+83xA4Hy/38+5551y+uSb77nfe1NVSJJe/V6z0QNIkibDoEtSEwZdkpow6JLUhEGXpCY2b9QL33zzzbVjx46NenlJelV67LHHnqqqqaWObVjQd+zYwezs7Ea9vCS9KiX5t+WOeclFkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmtiwO0WvRrLRE+ha5u9s0fXKM3RJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1MSgoCfZm+Rskrkkh5c4fleS+SSnR39+bfKjSpJWsupPW0yyCTgKvBu4AJxKMlNVjy9a+smqOrQGM0qSBhhyhr4HmKuqc1X1InAc2L+2Y0mSLteQoG8Fzo9tXxjtW+y9Sb6a5OEk25f6QkkOJplNMjs/P38F40qSljOpD0U/BeyoqrcBnwU+sdSiqjpWVdNVNT01NTWhl5YkwbCgXwTGz7i3jfa9rKqerqoXRpt/Dvz4ZMaTJA01JOingF1JdibZAhwAZsYXJPmhsc19wBOTG1GSNMSq3+VSVZeSHAJOApuAB6rqTJIjwGxVzQAfTrIPuAQ8A9y1hjNLkpaQ2qDfqDs9PV2zs7NX9Fx/SbRW4i+JVmdJHquq6aWOeaeoJDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUxKq/4ELSFfCH9msla/RD+z1Dl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJamJQ0JPsTXI2yVySwyuse2+SSjI9uRElSUOsGvQkm4CjwB3AbuDOJLuXWHcj8BHgS5MeUpK0uiFn6HuAuao6V1UvAseB/Uus+xhwD/DtCc4nSRpoSNC3AufHti+M9r0syW3A9qr69EpfKMnBJLNJZufn5y97WEnS8q76Q9EkrwE+Dvzuamur6lhVTVfV9NTU1NW+tCRpzJCgXwS2j21vG+17yY3ArcAXkjwJvAOY8YNRSVpfQ4J+CtiVZGeSLcABYOalg1X1XFXdXFU7qmoH8Ciwr6pm12RiSdKSVg16VV0CDgEngSeAh6rqTJIjSfat9YCSpGE2D1lUVSeAE4v23b3M2ndd/ViSpMvlnaKS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTQwKepK9Sc4mmUtyeInjH0rytSSnk3wxye7JjypJWsmqQU+yCTgK3AHsBu5cItgPVtWPVNWPAvcCH5/4pJKkFQ05Q98DzFXVuap6ETgO7B9fUFX/Nbb5eqAmN6IkaYjNA9ZsBc6PbV8A3r54UZLfBj4KbAF+bqkvlOQgcBDglltuudxZJUkrmNiHolV1tKreDPwe8AfLrDlWVdNVNT01NTWpl5YkMSzoF4HtY9vbRvuWcxz4pasZSpJ0+YYE/RSwK8nOJFuAA8DM+IIku8Y2fxH418mNKEkaYtVr6FV1Kckh4CSwCXigqs4kOQLMVtUMcCjJ7cB3gGeBD6zl0JKkVxryoShVdQI4sWjf3WOPPzLhuSRJl8k7RSWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaGBT0JHuTnE0yl+TwEsc/muTxJF9N8rkkb5r8qJKklawa9CSbgKPAHcBu4M4kuxct+wowXVVvAx4G7p30oJKklQ05Q98DzFXVuap6ETgO7B9fUFWfr6rnR5uPAtsmO6YkaTVDgr4VOD+2fWG0bzkfBB5Z6kCSg0lmk8zOz88Pn1KStKqJfiia5P3ANHDfUser6lhVTVfV9NTU1CRfWpKue5sHrLkIbB/b3jba9/8kuR34feCdVfXCZMaTJA015Az9FLAryc4kW4ADwMz4giQ/BtwP7Kuqb0x+TEnSalYNelVdAg4BJ4EngIeq6kySI0n2jZbdB9wA/HWS00lmlvlykqQ1MuSSC1V1AjixaN/dY49vn/BckqTL5J2iktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0MCnqSvUnOJplLcniJ4z+T5MtJLiV53+THlCStZtWgJ9kEHAXuAHYDdybZvWjZvwN3AQ9OekBJ0jCbB6zZA8xV1TmAJMeB/cDjLy2oqidHx767BjNKkgYYcsllK3B+bPvCaJ8k6Rqyrh+KJjmYZDbJ7Pz8/Hq+tCS1NyToF4HtY9vbRvsuW1Udq6rpqpqempq6ki8hSVrGkKCfAnYl2ZlkC3AAmFnbsSRJl2vVoFfVJeAQcBJ4Anioqs4kOZJkH0CSn0hyAfhl4P4kZ9ZyaEnSKw35Lheq6gRwYtG+u8cen2LhUowkaYN4p6gkNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpoYFPQke5OcTTKX5PASx1+b5JOj419KsmPSg0qSVrZq0JNsAo4CdwC7gTuT7F607IPAs1X1FuBPgXsmPagkaWVDztD3AHNVda6qXgSOA/sXrdkPfGL0+GHg55NkcmNKklazecCarcD5se0LwNuXW1NVl5I8B3w/8NT4oiQHgYOjzW8mOXslQ+sVbmbRf+vrmacS1yTfo+Ou7k36puUODAn6xFTVMeDYer7m9SDJbFVNb/Qc0nJ8j66PIZdcLgLbx7a3jfYtuSbJZuAm4OlJDChJGmZI0E8Bu5LsTLIFOADMLFozA3xg9Ph9wN9XVU1uTEnSala95DK6Jn4IOAlsAh6oqjNJjgCzVTUD/AXwl0nmgGdYiL7Wj5exdK3zPboO4om0JPXgnaKS1IRBl6QmDPo1KMkbkvzWFTzvRJI3rMVM0nKu9P06eu7vJPneSc90vfIa+jVo9LNw/q6qbl20f3NVXdqQoaRlLPd+HfjcJ4HpqvKmowlY1xuLNNifAG9Ochr4DvBt4FngrcAPJ/lbFr7v/3uAPxvdsPXy/xzADcAjwBeBn2LhPoH9VfWtdf576Pow/n79LPAN4FeA1wJ/U1V/mOT1wEMs3MeyCfgY8IPAG4HPJ3mqqn52Q6ZvxDP0a9D4GU+SdwGfBm6tqq+Pjn9fVT2T5HUs3Cfwzqp6elHQ51g48zmd5CFgpqr+av3/Nupu0fv1PSzci/IbQFi4R+VeYArYW1W/PnrOTVX1nGfok+U19FeHf3op5iMfTvLPwKMsnKnvWuI5X6+q06PHjwE71nZECYD3jP58BfgyC/+q3AV8DXh3knuS/HRVPbeBM7blJZdXh/956cHojP124Cer6vkkX2Dh0stiL4w9/l/gdWs5oDQS4I+r6v5XHEhuA34B+KMkn6uqI+s+XXOeoV+b/hu4cZljN7Hws+efT/JW4B3rN5a0pPH360ngV5PcAJBka5IfSPJG4PnRZb/7gNuWeK6ukmfo16DR9fB/SPIvwLeA/xw7/BngQ0meAM6ycNlF2jCL3q+PAA8C/zj6lQjfBN4PvAW4L8l3Wfig/zdHTz8GfCbJf/ih6NXzQ1FJasJLLpLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1IT/wfB39Qlmf+IDQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x=['train'], height=[np.average(iou_train)], color='blue', align='center')\n",
    "plt.bar(x=['test'], height=[np.average(iou_test)], color='red', align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24d5c9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f48a1462a00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(\n",
    "    x=[x + 1 for x in range(8)],\n",
    "    height=np.average(iou_train, 0),\n",
    "    color=\"blue\",\n",
    "    width=-0.4,\n",
    "    align=\"edge\",\n",
    "    label='train'\n",
    "\n",
    ")\n",
    "\n",
    "plt.bar(\n",
    "    x=[x + 1 for x in range(8)],\n",
    "    height=np.average(iou_test, 0),\n",
    "    color=\"red\",\n",
    "    width=0.4,\n",
    "    align=\"edge\",\n",
    "    label='test'\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "728120dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "  return np.concatenate([[x[0]], x[1], x[2]])\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    list(map(convert, [[name_model, np.average(iou_train, 0), np.average(iou_test, 0)]])),\n",
    "    columns=[\n",
    "        \"model\",\n",
    "        \"trainc1\",\n",
    "        \"trainc2\",\n",
    "        \"trainc3\",\n",
    "        \"trainc4\",\n",
    "        \"trainc5\",\n",
    "        \"trainc6\",\n",
    "        \"trainc7\",\n",
    "        \"trainc8\",\n",
    "        \"testc1\",\n",
    "        \"testc2\",\n",
    "        \"testc3\",\n",
    "        \"testc4\",\n",
    "        \"testc5\",\n",
    "        \"testc6\",\n",
    "        \"testc7\",\n",
    "        \"testc8\",\n",
    "    ],\n",
    ")\n",
    "second = results.pop('model')\n",
    "\n",
    "results.iloc[:, 0:] = results.iloc[:, 0:].astype('float')\n",
    "results['model'] = second\n",
    "results.to_csv(f'results_{name_model}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0beed873570eadf18b27de988f74387134654fe26ad0c1ed6b53170102862c4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tf21')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

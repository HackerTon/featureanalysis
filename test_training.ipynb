{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.transforms import RandomCrop, Resize, transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVIDDataset4K(Dataset):\n",
    "    def __init__(self, path, is_train=True):\n",
    "        directory = Path(path)\n",
    "        if is_train:\n",
    "            self.images = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_train/**/Images/*.png\")\n",
    "            ]\n",
    "            self.labels = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_train/**/Labels/*.png\")\n",
    "            ]\n",
    "        else:\n",
    "            self.images = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_val/**/Images/*.png\")\n",
    "            ]\n",
    "            self.labels = [\n",
    "                str(x.absolute()) for x in directory.glob(\"uavid_val/**/Labels/*.png\")\n",
    "            ]\n",
    "\n",
    "        if len(self.images) is not len(self.labels):\n",
    "            print(\"Number of images & label are not the same.\")\n",
    "            return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_image(image_path):\n",
    "        return read_image(image_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_image(image):\n",
    "        resizer = Resize([2160, 3840], antialias=\"True\")\n",
    "        return resizer(image)\n",
    "\n",
    "    @staticmethod\n",
    "    def label_0and1(label):\n",
    "        return label.type(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def image_0and1(image):\n",
    "        return (image / 255).type(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_label(label):\n",
    "        labels = []\n",
    "        labels.append((label[0] == 0) & (label[1] == 0) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 0) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 64) & (label[2] == 128))\n",
    "        labels.append((label[0] == 0) & (label[1] == 128) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 128) & (label[2] == 0))\n",
    "        labels.append((label[0] == 64) & (label[1] == 0) & (label[2] == 128))\n",
    "        labels.append((label[0] == 192) & (label[1] == 0) & (label[2] == 192))\n",
    "        labels.append((label[0] == 64) & (label[1] == 64) & (label[2] == 0))\n",
    "        return torch.stack(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.decode_image(self.images[index])\n",
    "        image = self.resize_image(image)\n",
    "        image = self.image_0and1(image)\n",
    "\n",
    "        label = self.decode_image(self.labels[index])\n",
    "        label = self.resize_image(label)\n",
    "        label = self.label_0and1(label)\n",
    "        label = self.mask_label(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAVIDDataset(Dataset):\n",
    "    def __init__(self, path, is_train=True):\n",
    "        directory = Path(path)\n",
    "        if is_train:\n",
    "            self.images = [\n",
    "                str(x.absolute()) for x in directory.glob(\"train/image/*.png\")\n",
    "            ]\n",
    "            self.labels = [\n",
    "                str(x.absolute()) for x in directory.glob(\"train/label/*.png\")\n",
    "            ]\n",
    "        else:\n",
    "            self.images = [\n",
    "                str(x.absolute()) for x in directory.glob(\"test/image/*.png\")\n",
    "            ]\n",
    "            self.labels = [\n",
    "                str(x.absolute()) for x in directory.glob(\"test/label/*.png\")\n",
    "            ]\n",
    "\n",
    "        if len(self.images) != len(self.labels):\n",
    "            print(\"Number of images & label are not the same.\")\n",
    "            return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_image(image_path):\n",
    "        return read_image(image_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_image(image):\n",
    "        resizer = Resize([2160, 3840], antialias=\"True\")\n",
    "        return resizer(image)\n",
    "\n",
    "    @staticmethod\n",
    "    def label_0and1(label):\n",
    "        return label.type(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def image_0and1(image):\n",
    "        return (image / 255).type(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_label(label):\n",
    "        labels = []\n",
    "        labels.append((label[0] == 0) & (label[1] == 0) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 0) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 64) & (label[2] == 128))\n",
    "        labels.append((label[0] == 0) & (label[1] == 128) & (label[2] == 0))\n",
    "        labels.append((label[0] == 128) & (label[1] == 128) & (label[2] == 0))\n",
    "        labels.append((label[0] == 64) & (label[1] == 0) & (label[2] == 128))\n",
    "        labels.append((label[0] == 192) & (label[1] == 0) & (label[2] == 192))\n",
    "        labels.append((label[0] == 64) & (label[1] == 64) & (label[2] == 0))\n",
    "        return torch.stack(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.decode_image(self.images[index])\n",
    "        i, j, h, w = RandomCrop.get_params(image, (256, 256))\n",
    "        image = self.image_0and1(image)\n",
    "        label = self.decode_image(self.labels[index])\n",
    "        label = self.mask_label(label)\n",
    "        label = self.label_0and1(label)\n",
    "\n",
    "        # Crop image and label\n",
    "        image = crop(image, i, j, h, w)\n",
    "        label = crop(label, i, j, h, w)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labels = [\n",
    "    \"background\",\n",
    "    \"building\",\n",
    "    \"road\",\n",
    "    \"tree\",\n",
    "    \"vegetation\",\n",
    "    \"moving_car\",\n",
    "    \"stationary_car\",\n",
    "    \"human\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = UAVIDDataset4K(path=\"data/uavid_v1.5_official_release_image\", is_train=True)\n",
    "training_data = UAVIDDataset(\n",
    "    path=\"/Users/babi/Programs/high_performance_analysis_system/data/processed_dataset/\",\n",
    "    is_train=True,\n",
    ")\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True)\n",
    "train_feature, train_label = next(iter(train_dataloader))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(torch.permute(train_feature[0], [1, 2, 0]))\n",
    "\n",
    "figure, axes = plt.subplots(8, 1, figsize=(16, 32))\n",
    "for i in range(8):\n",
    "    axes[i].set_axis_off()\n",
    "    axes[i].imshow(\n",
    "        torch.permute(train_label, [0, 2, 3, 1]).numpy()[0, ..., i],\n",
    "        cmap=\"gray\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    axes[i].text(5, 5, dataset_labels[i], bbox={\"facecolor\": \"white\", \"pad\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNETNetwork(nn.Module):\n",
    "    def __init__(self, numberClass):\n",
    "        super().__init__()\n",
    "        _resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = create_feature_extractor(\n",
    "            _resnet50,\n",
    "            {\n",
    "                \"layer1\": \"feat2\",\n",
    "                \"layer2\": \"feat3\",\n",
    "                \"layer3\": \"feat4\",\n",
    "                \"layer4\": \"feat5\",\n",
    "            },\n",
    "        )\n",
    "        self.upsampling_2x = nn.UpsamplingNearest2d(scale_factor=2)\n",
    "        self.upsampling_2x_bilinear = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=2048, out_channels=1024, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv6 = nn.Conv2d(\n",
    "            in_channels=1024, out_channels=512, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv7 = nn.Conv2d(\n",
    "            in_channels=512, out_channels=256, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv8 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=128, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.convfinal = nn.Conv2d(\n",
    "            in_channels=128, out_channels=numberClass, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_output = self.backbone(x)\n",
    "        feat2, feat3, feat4, feat5 = (\n",
    "            backbone_output[\"feat2\"],\n",
    "            backbone_output[\"feat3\"],\n",
    "            backbone_output[\"feat4\"],\n",
    "            backbone_output[\"feat5\"],\n",
    "        )\n",
    "        feat4to6 = self.upsampling_2x_bilinear(self.conv5(feat5).relu())\n",
    "        feat3to7 = self.upsampling_2x_bilinear(self.conv6(feat4 + feat4to6).relu())\n",
    "        feat2to8 = self.upsampling_2x_bilinear(self.conv7(feat3 + feat3to7).relu())\n",
    "        featout = self.upsampling_2x_bilinear(self.conv8(feat2 + feat2to8).relu())\n",
    "        return self.UpsamplingNearest2d(self.convfinal(featout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(\n",
    "    pred: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    epsilon=1e-9,\n",
    "):\n",
    "    pred_flat = pred.flatten()\n",
    "    target_flat = target.flatten()\n",
    "    nominator = 2 * torch.matmul(pred_flat, target_flat)\n",
    "    denominator = torch.sum(pred_flat) + torch.sum(target_flat)\n",
    "    return 1 - ((nominator + epsilon) / (denominator + epsilon))\n",
    "\n",
    "\n",
    "def total_loss(pred: torch.Tensor, target: torch.Tensor):\n",
    "    return torch.nn.functional.cross_entropy(\n",
    "        pred,\n",
    "        target,\n",
    "    ) + dice_loss(\n",
    "        pred.softmax(1),\n",
    "        target,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = UAVIDDataset(\n",
    "    path=\"/Users/babi/Programs/high_performance_analysis_system/data/processed_dataset/\",\n",
    "    is_train=True,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "model = UNETNetwork(numberClass=8).to(\"mps\")\n",
    "optimizer_sgd = torch.optim.SGD(params=model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]).to(\"mps\")\n",
    "timestamp = datetime.datetime.now().strftime(r\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(\"data/training/train_{}\".format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('data/savedmodel/49_model.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    for idx, data in enumerate(tqdm(train_dataloader)):\n",
    "        inputs: torch.Tensor\n",
    "        labels: torch.Tensor\n",
    "        inputs, labels = data\n",
    "        optimizer_sgd.zero_grad()\n",
    "\n",
    "        inputs = inputs.to(\"mps\")\n",
    "        labels = labels.to(\"mps\")\n",
    "\n",
    "        inputs = normalize(inputs)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = total_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_sgd.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if idx % 500 == 499:\n",
    "            current_training_sample = epoch * len(train_dataloader) + idx + 1\n",
    "            writer.add_scalar(\"Loss/train\", running_loss / 500, current_training_sample)\n",
    "            print(f\"Loss: {running_loss / 500}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_channels(image: torch.Tensor, colors: np.ndarray, is_predict: bool):\n",
    "    _, _, h, w = image.shape\n",
    "    output_image = np.zeros([h, w, 3], dtype=np.uint8)\n",
    "    for i in range(colors.shape[0]):\n",
    "        if is_predict:\n",
    "            mask = image[0, i] > 0.5\n",
    "        else:\n",
    "            mask = image[0, i] == 1\n",
    "        output_image[mask] = colors[i]\n",
    "    return output_image\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    input_image: torch.Tensor,\n",
    "    grouth_truth: torch.Tensor,\n",
    "    predicted: torch.Tensor,\n",
    "):\n",
    "    colors = np.array(\n",
    "        [\n",
    "            [0, 0, 0],\n",
    "            [128, 0, 0],\n",
    "            [128, 64, 128],\n",
    "            [0, 128, 0],\n",
    "            [128, 128, 0],\n",
    "            [64, 0, 128],\n",
    "            [192, 0, 192],\n",
    "            [0, 0, 128],\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    )\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 9), dpi=200)\n",
    "    legend_patches = [\n",
    "        patches.Patch(\n",
    "            color=np.concatenate([color / 255, [1]]),\n",
    "            label=dataset_labels[idx],\n",
    "        )\n",
    "        for idx, color in enumerate(colors)\n",
    "    ]\n",
    "    fig.legend(handles=legend_patches, bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    grouth_truth_image = combine_channels(grouth_truth, colors, False)\n",
    "    predicted_image = combine_channels(predicted, colors, True)\n",
    "    input_image = torch.permute(input_image[0], [1, 2, 0])\n",
    "\n",
    "    axes[0].set_axis_off()\n",
    "    axes[1].set_axis_off()\n",
    "    axes[2].set_axis_off()\n",
    "\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[1].set_title('Grouth Truth Image')\n",
    "    axes[2].set_title('Predicted Image')\n",
    "\n",
    "    axes[0].imshow(input_image)\n",
    "    axes[1].imshow(grouth_truth_image)\n",
    "    axes[2].imshow(predicted_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_data = UAVIDDataset(\n",
    "    path=\"/Users/babi/Programs/high_performance_analysis_system/data/processed_dataset/\",\n",
    "    is_train=False,\n",
    ")\n",
    "test_dataloader = DataLoader(training_data, batch_size=1, shuffle=False)\n",
    "test_feature, test_label = next(iter(test_dataloader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(normalize(test_feature.to(\"mps\")))\n",
    "    outputs = outputs.to(\"cpu\").softmax(axis=1)\n",
    "    visualize(test_feature, test_label, outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
